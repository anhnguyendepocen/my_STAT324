---
title: "Lecture 15: Two Sample Hypothesis Tests"
subtitle: "STAT 324"
author: "Ralph Trane"
institute: "University of Wisconsin--Madison<br><br>"
date: "Spring 2020"
output:
  xaringan::moon_reader:
    css: [../css/uwmadison.css, default-fonts, ../css/extra-classes.css]
    lib_dir: libs
    nature:
      titleSlideClass: [center, top, .title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:10'
      navigation:
        scroll: false
---
layout: true

# Two Independent Samples Hypothesis Test

---

```{r echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      dpi = 300, fig.height = 4, out.height = "400px",
                      fig.align = "center")
```

The horned lizard Phrynosoma mcalli is named for the fringe of spikes around the back of the head. It was thought that the spikes may provide the lizard protection from its primary predator, the loggerhead shrike, Lanius ludovicanus, though there was not much existing quantitative evidence to support this. Researchers were interested in comparing two populations: the population of dead lizards known to be killed by shrikes, and the population of live lizards from the same geographic location. Random samples were taken from each population. The longest spike was measured on each sampled lizard, in mm.

---

The fundamental question: is there, overall, a difference between the longest spike in the two populations?

--

In terms of means: is $\mu_\text{dead} = \mu_\text{alive}$ or not?

--

Some data:

```{r include = FALSE}
library(tidyverse)
dead <- c(17.65, 20.83, 24.59, 18.52, 21.40, 23.78, 20.36, 18.83, 21.83, 20.06)
alive <- c(23.76, 21.17, 26.13, 20.18, 23.01, 24.84, 19.34, 24.94, 27.14, 25.87, 18.95, 22.61)

lizards <- tibble(group = rep(c("dead", "alive"), times = c(length(dead), length(alive))),
                  size = c(dead, alive))

```

```{r}
DT::datatable(lizards, options = list(pageLength = 4))
```


---

The fundamental question: is there, overall, a difference between the longest spike in the two populations?

In terms of means: is $\mu_\text{dead} = \mu_\text{alive}$ or not?

.pull-left[
```{r lizards_boxplot, eval = FALSE}
ggplot(lizards,
       aes(x = group, y = size)) + 
  geom_boxplot(aes(fill = group),
               alpha = 0.2) + 
  geom_hline(data = lizards %>% 
               group_by(group) %>% 
               summarize(mean = mean(size)),
             aes(yintercept = mean, color = group)) + 
  geom_jitter(height = 0, width = 0.2, 
              aes(color = group)) +
  labs(color = "", fill = "")
```
]

.pull-right[
```{r echo = F, ref.label='lizards_boxplot', fig.width=4, fig.height=3, out.width = "400px", out.height = "300px"}
```
]

Are the lines so far apart that we reject the idea that the underlying true means are the same?

---

Since $\bar{X}_\text{dead}$ is expected to be close to $\mu_\text{dead}$, and $\bar{X}_\text{alive}$ is expected to be close to $\mu_\text{alive}$, $\bar{X}_\text{alive} - \bar{X}_\text{dead}$ is expected to be close to $\mu_\text{alive} - \mu_\text{dead}$. 

--

We can rephrase the question in terms of hypotheses:

$$H_0: \mu_\text{alive} - \mu_\text{dead} = 0 \qquad \text{vs.} \qquad H_A: \mu_\text{alive} - \mu_\text{dead} \neq 0$$

So the question is, is our observed difference in averages ( $\bar{X}_\text{alive} - \bar{X}_\text{dead}$ ) so far from $0$ that we no longer think that $\mu_\text{alive} - \mu_\text{dead} = 0$ (i.e. we reject the idea that the means are the same)?

How would we go about answering this question?

---

**IF** $\bar{X}_\text{alive} \sim N$ and $\bar{X}_\text{dead} \sim N$, then $\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim$
--
 $N$. 

--

**IF** $H_0$ is true, then $E(\bar{X}_\text{alive} - \bar{X}_\text{dead}) = E(\bar{X}_\text{alive}) - E(\bar{X}_\text{dead}) = \mu_\text{alive} - \mu_\text{dead} =$
--
 $0$.


--

So, **IF** $H_0$ is true, then $\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim N(0, ??)$. 

--

**IF** the two samples are independent of each other, $\bar{X}_\text{alive}$ is independent of $\bar{X}_\text{dead}$, so 

--

$$\text{Var}(\bar{X}_\text{alive} - \bar{X}_\text{dead}) = \text{Var}(\bar{X}_\text{alive}) + \text{Var}(\bar{X}_\text{dead}) = \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}$$

--

So, **IF** $H_0$ is true, then $\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim N\left(0, \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}\right)$. 

---

So, how do we judge if what we see is so far from the null hypothesis that we decide to reject it? 

--

By finding the probability of observing something more extreme if we were to repeat the experiment, **assuming the null hypothesis is true**!

--

**IF** $H_0$ is true, **and we know $\sigma_\text{alive}, \sigma_\text{dead}$**, this is pretty straight forward:

- look at the curve that is the distribution of the difference $\bar{X}_\text{alive} - \bar{X}_\text{dead}$, i.e. $N\left(0, \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}\right)$. 

--

- using quantiles:
    - find quantiles that cut-off $\alpha/2$ on each side.
    - reject if observed value of the difference is outside the cut-offs

--

- using p-value:
    - find probability of something "more extreme"
    - reject if smaller than $\alpha$

---

Using quantiles: reject if outside of dotted lines that cut-off $\alpha/2$ on each side.

```{r echo = F}
library(distributions3)
D <- Normal(0, 4)

plot_pdf(D) + 
  geom_auc(to = quantile(D, p = 0.025), fill = "grey70") +
  geom_auc(from = quantile(D, p = 0.975), fill = "grey70") +
  geom_vline(xintercept = quantile(D, p = c(0.025, 0.975)),
             linetype = "dashed", color = "red") +
  # annotate(x = 10, y = 0.1, label = "Area of each\n grey area is 0.025",
  #          #hjust = 1, 
  #          vjust = 1,
  #          geom = "label") +
  labs(x = "Difference in averages", y = 'Density') +
  theme_bw()
```

---

Using p-value: reject if area outside dotted lines is smaller than $\alpha$

```{r echo = F, fig.height = 4.5, fig.width = 7, out.height = "450px", out.width = "700px"}
D <- Normal(0, 4)

plot_pdf(D) + 
  geom_auc(to = -5, fill = "grey75") +
  geom_auc(from = 5, fill = "grey75") +
  geom_vline(data = tibble(x = c(-5, 5),
                           color = c("Mirrored value", "Observed value")),
             aes(xintercept = x,
                 color = color),
             linetype = "dashed") +
  labs(x = "Difference in averages", y = 'Density') +
  scale_color_manual("", values = c("grey45", "red")) +
  theme_bw() +
  theme(legend.position = "bottom")
```

---
 
Problem: we never know $\sigma_\text{dead}, \sigma_\text{alive}$!! 

--

In the one sample case, we got around this by considering $\frac{\bar{X} - \mu_0}{\widehat{\text{SD}}(\bar{X})} = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}$, which we know is $t_{n-1}$.

--

In the two sample case, we will use 

$$
  T = \frac{V - v_0}{\widehat{\text{SD}}(V)},
$$

where $V = \bar{X}_\text{alive} - \bar{X}_\text{dead}$, and (in the most general case) 

$$\widehat{\text{SD}}(V) = \sqrt{s^2_\text{alive}/n_\text{alive} + s^2_\text{dead}/n_\text{dead}}$$

As usual, **IF** $V \sim N$, and $H_0: v = v_0$, then  $T \sim t_\text{some appropriate df}$. Things get a bit more tricky here, though, as deciding the approriate df is not trivial.

---

In general, two scenarios:

**Scenario 1**: $\sigma_1^2 = \sigma_2^2$. 

When this is the case, we replace both by a common number, $\sigma_\text{pooled}^2$ (or simply $\sigma^2_p$ for convenience). 

Adding this extra bit of information means we can do slightly better in trying to estimate the variance in the two groups. Our best guess for the pooled variance is 

$$\hat{\sigma}^2_p = s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$$

---

$$\hat{\sigma}^2_p = s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$$

Intuition: 

* this is a *weighted average* of our two best guesses

* we have a best guess for group 1, best guess for group 2, so surely the "truth" must be somewhere between the two.

* the group with more data (i.e. more information) gets more weight

--

* if the means in the two groups were the same, the pooled standard deviation is actually the same as just treating the two groups as one.
    - cannot do this when means are different because of definition of standard deviation

--

We now have that $\text{Var}(V) = \frac{s_p^2}{n_1} + \frac{s_p^2}{n_2} = s_p^2 (1/n_1 + 1/n_2)$, and our test statistic will follow a $t_{n_1+n_2-2}$ distribution:

$$T = \frac{V - v_0}{s_p\sqrt{1/n_1 + 1/n_2}} \sim t_{n_1 + n_2 - 2}$$

---

**Scenario 2**: $\sigma_1^2 \neq \sigma_2^2$. 

In this case, we do not gain any insights, and so there's no adjustments we can make to the test statistic. 

It turns out that 

$$T = \frac{V - v_0}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \sim t_{\nu},$$

where 

$$
  \nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}
$$


--

In either case, we can find the distribution of $T$, and use this to either reject or not reject the null hypothesis!

---

Since we do not know $\sigma_1^2$ or $\sigma_2^2$, we use $s_1^2$ and $s_2^2$ to determine if we find it plausible that $\sigma_1^2 = \sigma_2^2$. 

The general rule of thumb: if $0.5 < \frac{s_1}{s_2} < 2$, then we are okay with assuming $\sigma_1^2 = \sigma_2^2$. 

--

In our lizards example:

```{r}
std_devs <- lizards %>% 
  group_by(group) %>% 
  summarize(s = sd(size),
            n = n()) 

std_devs
```

So, we will assume $\sigma_\text{dead}^2 = \sigma_\text{alive}^2$. 

---

That means we have to find $s_p^2$ for our test:

```
sp2 <- ((`r std_devs$n[1]` - 1)*`r std_devs$s[1]`^2 + (`r std_devs$n[2]` - 1)*`r std_devs$s[2]`^2)/(`r std_devs$n[1]` + `r std_devs$n[2]` - 2)
sp2
```


```{r echo = FALSE}
sp2 <- ((std_devs$n[1] - 1)*std_devs$s[1]^2 + (std_devs$n[2] - 1)*std_devs$s[2]^2)/(std_devs$n[1] + std_devs$n[2] - 2)
sp2
```

We can now calculate the observed value of the test statistic: 

.pull-left[
```{r}
means_and_ns <- lizards %>% 
  group_by(group) %>% 
  summarize(means = mean(size),
            n = n())
means_and_ns
```
]

.pull-right[
$$
\begin{aligned}
T_\text{obs} &= \frac{V - v_0}{\widehat{SD}(V)} \\
             &= \frac{\bar{X}_\text{alive} - \bar{X}_\text{dead}}{s_p\sqrt{1/n_\text{alive} + 1/n_\text{dead}}} \\
             &= \frac{`r means_and_ns[['means']][1]` - `r means_and_ns[['means']][2]`}{`r sqrt(sp2)`\sqrt{1/`r means_and_ns[['n']][1]` + 1/`r means_and_ns[['n']][2]`}} \\
             &= `r means_and_ns %>% summarize(t = (means[1] - means[2])/(sqrt(sp2*(sum(1/n))))) %>% pull(t)`
\end{aligned}
$$
]


---

We compare this to a $t_{n_\text{alive} + n_\text{dead} - 2}$ distribution to find the p-value:

```{r echo = FALSE, fig.height = 3, out.height = "300px"}
plot_pdf(StudentsT(df = 20)) +
  geom_vline(xintercept = 2.19,
             linetype = "dashed",
             color = "red")
```

---

We compare this to a $t_{n_\text{alive} + n_\text{dead} - 2}$ distribution to find the p-value:

```{r echo = FALSE, fig.height = 3, out.height = "300px"}
plot_pdf(StudentsT(df = 20)) +
  geom_auc(to = -2.19) +
  geom_auc(from = 2.19) +
  geom_vline(xintercept = 2.19,
             linetype = "dashed",
             color = "red")
```


```{r}
T_20 <- StudentsT(20)
2*(1 - cdf(T_20, 2.192))
```

---

```{r}
t.test(data = lizards,
       size ~ group, var.equal = TRUE)
```


---

Which decade had better movies: the 90s of 2010s? 

```{r}
movies_orig <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/movies.csv')
movies <- movies_orig %>% 
  mutate(decade = as.numeric(str_sub(thtr_rel_year, start = 3, end = 3))*10)

DT::datatable(movies, options = list(pageLength = 3, scrollX = TRUE))
```


---

We'll use `critics_score` as our criteria for "good movie". This dataset contains a random sample of movies from IMDB. 

We will test $H_0: \mu_{90} = \mu_{10}$ against $H_A: \mu_{90} \neq \mu_{10}$ using $\alpha = 0.01$.

First, need to find out if we can assume equal variances:

```{r}
movies %>% 
  group_by(decade) %>% 
  summarize(s = sd(critics_score))
```

We can! 

---

So, let's find means, $s_p$, and $T_\text{obs}$. 

.pull-left[
```{r}
movies %>% filter(decade %in% c(90, 10)) %>% 
  group_by(decade) %>% 
  summarize(means = mean(critics_score),
            s = sd(critics_score),
            n = n())

sp2 <- ((102 - 1)*27.1^2 + (161 - 1)*28.79^2)/(102 + 161 - 2)
sp2
```
]

.pull-right[

$$
\begin{aligned}
  T_\text{obs} &= \frac{V - v_0}{\widehat{SD}(V)} \\
               &= \frac{\bar{x}_{10} - \bar{x}_{90}}{s_p \sqrt{1/n_{10} + 1/n_{90}}} \\
               &= \frac{62.42 - 52.88}{\sqrt{792.31} \sqrt{1/102 + 1/161}} \\
               &= `r (imdb_t_obs <- (62.42 - 52.88)/(sqrt(792.31)*sqrt(1/102 + 1/161)))`
\end{aligned}
$$
]

---

**If** $H_0$ is true, $T \sim t_{n_{10} + n_{90} - 2}$. 

```{r echo = FALSE}
plot_pdf(StudentsT(102+161-2)) + 
  geom_vline(xintercept = imdb_t_obs, linetype = "dashed", color = "red")
```

---

Since we are testing against $H_A: \mu_{90} \neq \mu_{10}$, we need to consider both tails:

```{r echo = FALSE}
plot_pdf(StudentsT(102+161-2)) + 
  geom_auc(to = -abs(imdb_t_obs)) + 
  geom_auc(from = abs(imdb_t_obs)) + 
  geom_vline(xintercept = c(-1,1)*imdb_t_obs, linetype = "dashed", color = "red")
```

---

```
T_261 <- StudentsT(261)

2*(1-cdf(T_261, `r imdb_t_obs`))
```

```{r echo = FALSE}
T_261 <- StudentsT(261)

2*(1-cdf(T_261, imdb_t_obs))
```

--

Since the p-value is smaller than $\alpha$, we reject the null hypothesis. There seems to be a difference in mean critic score between 1990s movies and 2010s movies. 

--

The quick way:

```{r}
t.test(data = filter(movies, decade %in% c(10, 90)),
       critics_score ~ decade, var.equal = TRUE)
```


---


Concrete used for roadways or buildings is often reinforced with a material that is placed inside the setting concrete. A common example of this is called ’rebar’ which is short for ’reinforcing bar’ and is usually made out of steel. You can often see rebar poking out of the concrete of demolished buildings. It is desireable that the reinforcing material is strong and corrosion resistant. Steel is strong, but tends to corrode over time, so experiments were conducted to test two corrosion resistant materials, one made of fiberglass and the other made of carbon.

Eight beams with fiberglass reinforcement, and 11 beams with carbon reinforcement were poured, and each was then subjected to a load test, which measures the strength of the beam. Strength is measured in kN (kiloNewtons), which is a measure of the force required to break the beam.

Research question: is there any difference in strength?

--

I.e., want to test $H_0: \mu_\text{fiber} = \mu_\text{carbon}$ vs. $H_A: \mu_\text{fiber} \neq \mu_\text{carbon}$.

(Or, equivalently, $H_0: \mu_\text{fiber} - \mu_\text{carbon} = 0$ vs. $H_0: \mu_\text{fiber} - \mu_\text{carbon} \neq 0$.)

---

```{r include = F}
fiberglass <- c(38.3, 29.6, 33.4, 33.6, 30.7, 32.7, 34.6, 32.3)
carbon <- c(48.8, 38.0, 42.2, 45.1, 32.8, 47.2, 50.6, 44.0, 43.9, 40.4, 45.8)

rebars <- tibble(type = rep(c("fiberglass", "carbon"), times = c(length(fiberglass), length(carbon))),
                 strength = c(fiberglass, carbon))
```

```{r}
DT::datatable(rebars, options = list(pageLength = 5))
```

---

.pull-left[
```{r rebars_boxplot, eval = F}
ggplot(rebars,
       aes(x = type, y = strength)) + 
  geom_boxplot() +
  geom_jitter(width = 0.2, height = 0,
              aes(color = type)) +
  geom_hline(data = rebars %>% 
               group_by(type) %>% 
               summarize(means = mean(strength)),
             aes(yintercept = means,
                 color = type)) + 
  theme(legend.position = "bottom")
```
]

.pull-right[
```{r ref.label='rebars_boxplot', echo = F, fig.width = 3, out.width = "300px", fig.height = 5, out.height = "500px"}
```
]

---

First, need to decide if we can assume equal variances:

.pull-left[
```{r rebars_sds, eval = FALSE}
rebars %>% 
  group_by(type) %>% 
  summarize(s = sd(strength))
```
]

.pull-right[
```{r ref.label="rebars_sds", echo = FALSE}
```
]

Since the standard deviation of the strength of carbon bars is more than twice that of the fiberglass bars, we cannot assume that the variances are the same! 

---

The simple step first: find the observed test statistic: 

```{r}
rebars %>% 
  group_by(type) %>% 
  summarize(means = mean(strength),
            s = sd(strength),
            n = n()) %>% print() %>% # code for first table ends here
  ungroup() %>% 
  summarize(T_obs = diff(means)/(sqrt(sum(s^2/n))))
```

---

"By hand":

$$
\begin{aligned}
  T_\text{obs} &= \frac{V - v_0}{\widehat{\text{SD}}(V)} \\
               &= \frac{\bar{x}_\text{fiber} - \bar{x}_\text{carbon}}{\sqrt{s^2_\text{fiber}/n_\text{fiber} + s^2_\text{carbon}/n_\text{carbon}}} \\
               &= \frac{33.15 - 43.53}{\sqrt{2.63^2/8 + 5.06^2/11}} \\
               &= `r (rebar_t_obs <- (33.15 - 43.53)/sqrt(2.63^2/8 + 5.06^2/11))`
\end{aligned}
$$

---

Next, we need to determine the degrees of freedom. Since we cannot assume equal variances, we need to use what is known as Satterthwaite's approximation:

$$
\begin{aligned}
  \nu &= \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}} \\
      &= \frac{\left(\frac{2.63^2}{8} + \frac{5.06^2}{11} \right)^2}{\frac{(2.63^2/8)^2}{8-1} + \frac{(5.06^2/11)^2}{11-1}} \\
      &= `r (sw_df <- (2.63^2/8 + 5.06^2/11)^2/((2.63^2/8)^2/(8-1) + (5.06^2/11)^2/(11-1)))`
\end{aligned}
$$

When used to determine the correct df, we always round down. So, $\nu = `r floor(sw_df)`$.

---

So, if $H_0$ is true, $T \sim t_{`r floor(sw_df)`}$. 

```{r echo = F}
plot_pdf(StudentsT(floor(sw_df)),
         limits = c(rebar_t_obs*1.1, Inf)) +
  geom_vline(xintercept = rebar_t_obs,
             color = "red", linetype = "dashed")
```

---

Since $H_A: \mu_\text{fiberglass} \neq \mu_\text{carbon}$, we find the p-value as twice the area to the left of the observed value:

```
T_15 <- StudentsT(15)
2*cdf(T_15, `r rebar_t_obs`)
```

```{r echo = F}
T_15 <- StudentsT(15)
2*cdf(T_15, rebar_t_obs)
```

The p-value is very, very small, so we reject the null hypothesis. It seems that there is a difference in mean strength between the two materials.

---

**Summary**

A Two Sample T-test with Independent Samples uses the test statistic $T = \frac{V - v_0}{\widehat{\text{SD}}(V)}$, where $V = \bar{X}_1 - \bar{X}_2$ and $v_0$ is the "null value" ( $H_0: V = v_0$; usually $v_0 = 0$ ). 


.pull-left[

If $0.5 < \frac{s_1}{s_2} < 2$, we assume equal variances $\sigma_1^2 = \sigma_2^2$. In this case, 

* $\widehat{\text{SD}}(V) = s_p \sqrt{1/n_1 + 1/n_2}$, where $$s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}$$

* if $H_0$ is true, $T \sim t_{n_1 + n_2 - 2}$. 
]

.pull-right[

If we cannot assume equal variances,

* $\widehat{\text{SD}}(V) = \sqrt{s_1^2/n_1 + s_2^2/n_2}$

* if $H_0$ is true, $T \sim t_{\nu}$ where $$\nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}$$
]

---
layout: true

# Two Independent Sample T Confidence Interval

---

As always, simply performing a test does not give us a lot of information. It simply answers the question "is this one value plausibly the true value?", where we would much rather ask "what values could be the true value?" 

Fortunately, we can construct confidence intervals for the difference in means from the two sample t-test. 

1. Find the distribution of $T = \frac{V - v_0}{\widehat{\text{SD}}(V)}$ **assuming** the null hypothesis is true.

2. Find cut-offs (i.e. quantiles) in said distribution such that we have $\alpha$ outside the cut-offs.

3. Create the confidence interval as $V_\text{obs} \pm t_{\text{df}, \alpha/2} \widehat{\text{SD}}(V)$.

---

**Example: Fiber vs Carbon**

We already established that the variances are not equal, and found $\nu = 15$. So, we can find a $90\%$ CI as follows:

.pull-left[
```{r}
rebars %>% 
  group_by(type) %>% 
  summarize(mean = mean(strength),
            s = sd(strength),
            n = n())
```

```{r}
quantile(T_15, 1-0.1/2)
```

]

.pull-right[
.small[
$$
\begin{aligned}
  V_\text{obs} &\pm t_{15, 0.1/2}\widehat{\text{SD}}(V) \\
  &= \bar{X}_\text{fiber} - \bar{X}_\text{carbon} \pm t_{15, 0.1/2} \sqrt{\frac{s^2_\text{fiber}}{n_\text{fiber}} + \frac{s^2_\text{carbon}}{n_\text{carbon}}} \\ 
  &= 33.15 - 43.53 \pm 1.75 \sqrt{2.63^2/8 + 5.06^2/11} \\
  &= -10.38 \pm 3.13
\end{aligned}
$$
]

So the 90% CI for the difference in means is [-13.51, -7.25].
]

---

**Example: IMDB Movies**

We saw that we can reasonably assume equal variances, and found the pooled variance. We can now find a $99\%$ CI.

.pull-left[
```{r}
movies %>% 
  filter(decade %in% c(90, 10)) %>% 
  group_by(decade) %>% 
  summarize(means = mean(critics_score),
            s = sd(critics_score),
            n = n())

((102-1)*27.1^2 + (161-1)*28.8^2)/(102 + 161 - 2)
```
]

.pull-right[
```{r}
quantile(T_261, 1 - 0.01/2)
```

.small[
$$
\begin{aligned}
  V_\text{obs} &\pm t_{261, 0.01/2}\widehat{\text{SD}}(V) \\
  &= \bar{X}_\text{10} - \bar{X}_\text{90} \pm t_{261, 0.01/2} \sqrt{s_p^2 \left(\frac{1}{n_\text{10}} + \frac{1}{n_\text{90}}\right)} \\ 
  &= 62.4 - 52.9 \pm 2.59 \sqrt{792.31\cdot (1/102 + 1/161)} \\
  &= 9.5 \pm 9.23
\end{aligned}
$$
]

So the 99% CI for the difference in means is [0.27, 18.73].
]
