<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 14: Power and Sample Size Calculations</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ralph Trane" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="libs/jquery-1.12.4/jquery.min.js"></script>
    <link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding-0.9/datatables.js"></script>
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core-1.10.19/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../css/uwmadison.css" type="text/css" />
    <link rel="stylesheet" href="../css/extra-classes.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, .title-slide, title-slide

# Lecture 14: Power and Sample Size Calculations
## STAT 324
### Ralph Trane
### University of Wisconsinâ€“Madison<br><br>
### Spring 2020

---

layout: true

# Power/Type II Error Rate

---
 
Recall: for every hypothesis test, there is a conclusion. For every conclusion, one of three things will happen:

* we make the right decision
    - i.e. reject when `\(H_0\)` is false, do not reject when `\(H_0\)` is true

* we make a type I error
    - i.e. reject when `\(H_0\)` is actually true

* we make a type II error
    - i.e. fail to reject when `\(H_0\)` is actually false


---

We have full control over the type I error rate - it is exactly `\(\alpha\)`. To see this, say we are testing `\(H_0: \mu = \mu_0\)` against `\(H_A: \mu \neq \mu_0\)`. 

--

`$$\begin{aligned}
  P(\text{reject } H_0\ |\ H_0 \text{ true}) &amp;= P(T \text{ very far from 0}\ |\ H_0 \text{ true}) \\
                                             &amp;= P(T &gt; t_{n-1, \alpha/2}\ |\ H_0 \text{ true}) + P(T &lt; -t_{n-1, \alpha/2}\ |\ H_0 \text{ true})\\
                                             &amp;= \alpha
\end{aligned}$$`



&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-2-1.png" height="250px" style="display: block; margin: auto;" /&gt;


Similarly can be done for the two other alternative hypotheses.

---

The Type II error rate is a bit more tricky: `\(P(\text{type II}) = P(\text{fail to reject } H_0\ |\ H_0 \text{ false})\)`. 

--

To get a better sense of how this works, we will consider a simple case that we haven't talked about yet: one sample hypothesis test with known variance `\(\sigma^2\)`. 

Remember: good old paint thickness data.


```r
library(tidyverse); library(distributions3)
paint_thickness &lt;- tibble(
  thickness = c(1.29, 1.12, 0.88, 1.65, 1.48, 1.59, 1.04, 0.83, 
                1.76, 1.31, 0.88, 1.71, 1.83, 1.09, 1.62, 1.49)
)
```




Let us assume that `\(\bar{X} \sim N\)`, and that we know the true variance `\(\sigma^2 = 0.115\)`.

---

When testing `\(H_0: \mu = 1.5\)` against `\(H_A: \mu \neq 1.5\)` using `\(\alpha = 0.05\)`, we would reject when `\(\bar{x}_\text{obs}\)` is very, very far from `\(1.5\)`.  

--

Actually, we reject when `\(\bar{x}_\text{obs}\)` is so far from `\(1.5\)` that the probability of `\(\bar{X}\)` being even further from `\(1.5\)` is less than `\(0.05\)`: `\(2\cdot P(\bar{X} &gt; |\bar{x}_\text{obs}|\ |\ H_0 \text{ true}) &lt; 0.05\)`.

---

When `\(H_0\)` is true, `\(\bar{X} \sim N(1.5, 0.115/16)\)`. So, when `\(H_0\)` is true, `\(\bar{X}\)` follows this distribution:

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-5-1.png" height="400px" style="display: block; margin: auto;" /&gt;

---

Will reject when `\(\bar{X}\)` outside red dotted lines. 

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-6-1.png" height="400px" style="display: block; margin: auto;" /&gt;

Type II error rate has to do with what happens when the *alternative* is true: `\(P(\text{fail to reject } H_0\ |\ H_0 \text{ not true}) = P(\text{fail to reject } H_0\ |\ H_A \text{ true})\)`.

Power = 1 - Type II error rate.

---

Assuming we know the true `\(\sigma^2 = 0.115\)`, and that the null hypothesis is true, then `\(\bar{X} \sim N(1.5, 0.115/16)\)`. 

So, we reject when `\(\bar{X}\)` outside red dotted lines. I.e. if `\(0.025 &gt; P\left(\bar{X} &gt; |\bar{x}_\text{obs}|\right)\)`. 


```r
X_H0 &lt;- Normal(mu = 1.5, sigma = sqrt(0.115/16))

quantile(X_H0, c(0.025, 0.975))
```

```
## [1] 1.333836 1.666164
```


So, we reject when `\(\bar{x} &lt; 1.33\)` or `\(\bar{x} &gt; 1.66\)`. 

---

But what if the true mean is NOT `\(1.5\)`? What if, instead, it is `\(1.3\)`? What is `\(P(\text{type II error})\)`? 

--

To find the probability of rejecting, we need to look at a different curve, because if `\(\mu = 1.3\)`, then `\(\bar{X} \sim N(1.3, 0.115/16)\)`. 

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-8-1.png" height="400px" style="display: block; margin: auto;" /&gt;

---

But what if the true mean is NOT `\(1.5\)`? What if, instead, it is `\(1.3\)`? What is `\(P(\text{type II error})\)`? 

To find the probability of rejecting, we need to look at a different curve, because if `\(\mu = 1.3\)`, then `\(\bar{X} \sim N(1.3, 0.115/16)\)`. 

$$
`\begin{aligned}
  P(\text{type II error}) &amp;= P(\text{fail to reject } |\ \mu = 1.3) \\
                          &amp;= P(1.33 &lt; \bar{X} &lt; 1.66\ |\ \mu = 1.3) \\
                          &amp;= P(\bar{X} &lt; 1.66\ |\ \mu = 1.3) - P(\bar{X} &lt; 1.33\ |\ \mu = 1.3)
\end{aligned}`
$$


```r
X_HA &lt;- Normal(1.3, sqrt(0.115/16))

cdf(X_HA, 1.66) - cdf(X_HA, 1.33)
```

```
## [1] 0.3617108
```

--

That is, **IF** the true mean is `\(1.3\)`, we would fail to reject the idea that `\(\mu = 1.5\)` about `\(37\%\)` of the time. 

Or, similarly, we would only reject `\(\mu = 1.5\)` about `\(63\%\)` of the time.

--

What can we do to do better? 
--
 Increase sample size! 

---

What sample size do we need to be able to distinguish between `\(\mu = 1.5\)` and `\(\mu = 1.3\)` most of the time? Say, `\(80\%\)` of the time? 

That is, what should `\(n\)` be such that `\(P(\text{reject } H_0: \mu = 1.5\ |\ \mu = 1.3) = 0.8\)`? 

--

More general, if we are testing `\(H_0: \mu = \mu_0\)` against `\(H_A: \mu \neq \mu_0\)` at significance level `\(\alpha\)`, what sample size is needed to make sure that `\(P(\text{reject } H_0\ |\ \mu = \mu_A) = 1 - \beta\)`?

--

Turns out, this is approximately 

`\begin{equation}
  n \approx \left(\frac{\sigma(z_{\alpha/2} + z_\beta)}{\mu_0 - \mu_A}\right)^2
\end{equation}`

In our specific example, `\(\sigma = 0.34\)`, `\(\alpha = 0.05\)`, `\(\beta = 0.2\)`, `\(\mu_0 = 1.5\)`, and `\(\mu_A = 1.3\)`. 

---

We can find `\(z_{0.025}\)` and `\(z_{0.2}\)`:


```r
Z &lt;- Normal()
quantile(Z, c(1-0.025, 1-0.2))
```

```
## [1] 1.9599640 0.8416212
```

So, `\(n \approx \left(\frac{0.34(1.96 + 0.84)}{1.5 - 1.3} \right)^2 = 22.6576\)`.

To have `\(80\%\)` power to reject `\(1.5\)` as the true mean if the true mean is in fact `\(1.3\)`, we would need 23 samples. 

---

Note: 

* sample size depends only on standard deviation, `\(\alpha\)`, `\(\beta\)`, and *difference* between true and hypothesized means. 

* in practice, it goes as follows:
--

  - researchers pick desired `\(\alpha\)` and `\(\beta\)`
--

  - from previous, well-done experiments, a solid estimate of `\(\sigma\)` is obtained 
--

  - from expert knowledge, a "minimal interesting difference" is chosen (i.e. `\(\mu_0 - \mu_A\)`)
--

  - based on this, needed sample size is determined.


---

* Based on the data at hand, the engineers at the car manufacturer want to design a new study. 

* They want to test `\(H_0: \mu = 1.5\)` vs `\(H_A: \mu \neq 1.5\)` using `\(\alpha = 0.01\)`

* What sample size is needed to make sure they have `\(80\%\)` power to detect a difference of `\(0.4\)`? 

    - i.e. if the true mean is `\(0.4\)` from their null hypothesis, they want a `\(80\%\)` chance of rejecting the null

---

Based on the data, `\(\sigma^2 = 0.115\)`. So, if the null hypothesis is true, `\(\bar{X} \sim N(1.5, 0.115/n)\)`. 

If the alternative is true (say, `\(\mu = 1.9\)`), then `\(\bar{X} \sim N(1.9, 0.115/n)\)`. What should `\(n\)` be such that the area under the TRUE curve in the rejection region is 0.8)

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-11-1.png" height="400px" style="display: block; margin: auto;" /&gt;

---

Using formula above:

$$
  n \approx \left(\frac{0.34(2.576 + 0.842)}{0.4} \right)^2 = 8.4407681
$$

So we would need a sample size of `\(9\)` to achieve the desired power to detect a difference of `\(0.4\)`. 

---

Sometimes, the sample size is limited by other resources such as time or money. A natural question to ask is then, what power do we have to detect a certain difference, given the sample size we can afford?

--

We want to test `\(H_0: \mu = 1.5\)` against `\(H_A: \mu \neq 1.5\)` using `\(\alpha = 0.1\)`. With a sample size of `\(5\)`, what power do we have to detect a difference of `\(0.3\)` if the true variance is `\(\sigma^2 = 0.115\)`?

--

Step 1: find rejection region. 

- we do this **assuming `\(H_0\)` is true**. I.e. what value of `\(\bar{x}_\text{obs}\)` would lead us to reject `\(\mu = 1.5\)`?
--

- we reject when `\(P(\text{more extreme}\ |\ H_0) &lt; 0.1\)`. I.e. reject when `\(\bar{x}_{\text{obs}}\)` is smaller than the `\(0.05\)`th quantile or larger than the `\(0.95\)`th of the distributions of `\(\bar{X}\)`

```
Xbar &lt;- Normal(1.5, sqrt(0.115/5))
quantile(Xbar, 1-0.05)
```

```
## [1] 1.25055 1.74945
```
--

- so RR is `\((-\infty, 1.25055]\)` and `\([1.74945, \infty)\)`. 

---

Step 2: What is the probability of `\(\bar{X}\)` being in the RR **IF** the *alternative* is true? 
--

- if the alternative is a difference of `\(0.3\)` from the null, then `\(\mu = 1.2\)` or `\(\mu = 1.8\)`. Because of the symmetry of the normal distribution, which you pick won't change the result
--

- so, say that the alternative is true such that `\(\mu = 1.8\)`. Then what is `\(P(\bar{X} \text{ in } RR)\)`?
--

- `\(P(\bar{X} &lt; 1.25055\ |\ \mu = 1.8) + P(\bar{X} &gt; 1.74945\ | \mu = 1.8)\)`

```
X_HA &lt;- Normal(1.8, sqrt(0.115/5))
cdf(X_HA, 1.25055) + (1 - cdf(X_HA, 1.74945))
```

```
## [1] 0.6306981
```

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-14-1.png" height="200px" style="display: block; margin: auto;" /&gt;

---
layout: true

# Two Independent Samples Hypothesis Test

---

--

The horned lizard Phrynosoma mcalli is named for the fringe of spikes around the back of the head. It was thought that the spikes may provide the lizard protection from its primary predator, the loggerhead shrike, Lanius ludovicanus, though there was not much existing quantitative evidence to support this. Researchers were interested in comparing two populations: the population of dead lizards known to be killed by shrikes, and the population of live lizards from the same geographic location. Random samples were taken from each population. The longest spike was measured on each sampled lizard, in mm.

---

The fundamental question: is there, overall, a difference between the longest spike in the two populations?

--

In terms of means: is `\(\mu_\text{dead} = \mu_\text{alive}\)` or not?

--

Some data:




```r
DT::datatable(lizards, options = list(pageLength = 4))
```

<div id="htmlwidget-7fc0f95aa732c80a60cf" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7fc0f95aa732c80a60cf">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22"],["dead","dead","dead","dead","dead","dead","dead","dead","dead","dead","alive","alive","alive","alive","alive","alive","alive","alive","alive","alive","alive","alive"],[17.65,20.83,24.59,18.52,21.4,23.78,20.36,18.83,21.83,20.06,23.76,21.17,26.13,20.18,23.01,24.84,19.34,24.94,27.14,25.87,18.95,22.61]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>group<\/th>\n      <th>size<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":4,"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[4,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>


---

The fundamental question: is there, overall, a difference between the longest spike in the two populations?

In terms of means: is `\(\mu_\text{dead} = \mu_\text{alive}\)` or not?

.pull-left[

```r
ggplot(lizards,
       aes(x = group, y = size)) + 
  geom_boxplot(aes(fill = group),
               alpha = 0.2) + 
  geom_hline(data = lizards %&gt;% 
               group_by(group) %&gt;% 
               summarize(mean = mean(size)),
             aes(yintercept = mean, color = group)) + 
  geom_jitter(height = 0, width = 0.2, 
              aes(color = group)) +
  labs(color = "", fill = "")
```
]

.pull-right[
&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-17-1.png" width="400px" height="300px" style="display: block; margin: auto;" /&gt;
]

Are the lines so far apart that we reject the idea that the underlying true means are the same?

---

Since `\(\bar{X}_\text{dead}\)` is expected to be close to `\(\mu_\text{dead}\)`, and `\(\bar{X}_\text{alive}\)` is expected to be close to `\(\mu_\text{alive}\)`, `\(\bar{X}_\text{alive} - \bar{X}_\text{dead}\)` is expected to be close to `\(\mu_\text{alive} - \mu_\text{dead}\)`. 

--

We can rephrase the question in terms of hypotheses:

`$$H_0: \mu_\text{alive} - \mu_\text{dead} = 0 \qquad \text{vs.} \qquad H_A: \mu_\text{alive} - \mu_\text{dead} \neq 0$$`

So the question is, is our observed difference in averages ( `\(\bar{X}_\text{alive} - \bar{X}_\text{dead}\)` ) so far from `\(0\)` that we no longer think that `\(\mu_\text{alive} - \mu_\text{dead} = 0\)` (i.e. we reject the idea that the means are the same)?

How would we go about answering this question?

---

**IF** `\(\bar{X}_\text{alive} \sim N\)` and `\(\bar{X}_\text{dead} \sim N\)`, then `\(\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim\)`
--
 `\(N\)`. 

--

**IF** `\(H_0\)` is true, then `\(E(\bar{X}_\text{alive} - \bar{X}_\text{dead}) = E(\bar{X}_\text{alive}) - E(\bar{X}_\text{dead}) = \mu_\text{alive} - \mu_\text{dead} =\)`
--
 `\(0\)`.


--

So, **IF** `\(H_0\)` is true, then `\(\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim N(0, ??)\)`. 

--

**IF** the two samples are independent of each other, `\(\bar{X}_\text{alive}\)` is independent of `\(\bar{X}_\text{dead}\)`, so 

--

`$$\text{Var}(\bar{X}_\text{alive} - \bar{X}_\text{dead}) = \text{Var}(\bar{X}_\text{alive}) + \text{Var}(\bar{X}_\text{dead}) = \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}$$`

--

So, **IF** `\(H_0\)` is true, then `\(\bar{X}_\text{alive} - \bar{X}_\text{dead} \sim N\left(0, \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}\right)\)`. 

---

So, how do we judge if what we see are so far from the null hypothesis that we decide to reject it? 

--

By finding the probability of observing something more extreme if we were to repeat the experiment, **assuming the null hypothesis is true**!

--

**IF** `\(H_0\)` is true, **and we know `\(\sigma_\text{alive}, \sigma_\text{dead}\)`**, this is pretty straight forward:

- look at the curve that is the distribution of the difference `\(\bar{X}_\text{alive} - \bar{X}_\text{dead}\)`, i.e. `\(N\left(0, \frac{\sigma^2_\text{alive}}{n_\text{alive}} + \frac{\sigma^2_\text{dead}}{n_\text{dead}}\right)\)`. 

--

- using quantiles:
    - find quantiles that cut-off `\(\alpha/2\)` on each side.
    - reject if observed value of the difference is outside the cut-offs

--

- using p-value:
    - find probability of something "more extreme"
    - reject if smaller than `\(\alpha\)`

---

Using quantiles: reject if outside of dotted lines that cut-off `\(\alpha/2\)` on each side.

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-18-1.png" height="400px" style="display: block; margin: auto;" /&gt;

---

Using p-value: reject if area outside dotted lines is smaller than `\(\alpha\)`

&lt;img src="lec14_slides_files/figure-html/unnamed-chunk-19-1.png" width="700px" height="450px" style="display: block; margin: auto;" /&gt;

---
 
Problem: we never know `\(\sigma_\text{dead}, \sigma_\text{alive}\)`!! 

--

In the one sample case, we got around this by considering `\(\frac{\bar{X} - \mu_0}{\widehat{\text{SD}}(\bar{X})} = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}\)`, which we know is `\(t_{n-1}\)`.

--

In the two sample case, we will use 

$$
  T = \frac{V - v_0}{\widehat{\text{SD}}(V)},
$$

where `\(V = \bar{X}_\text{dead} - \bar{X}_\text{alive}\)`, and (in the most general case) 

`$$\widehat{\text{SD}}(V) = \sqrt{s^2_\text{dead}/n_\text{dead} + s^2_\text{alive}/n_\text{alive}}$$`

As usual, **IF** `\(V \sim N\)`, and `\(H_0: v = v_0\)`, then  `\(T \sim t_\text{some appropriate df}\)`. Things get a bit more tricky here, though, as deciding the approriate df is not trivial.

---

In general, two scenarios:

**Scenario 1**: `\(\sigma_1^2 = \sigma_2^2\)`. 

When this is the case, we replace both by a common number, `\(\sigma_\text{pooled}^2\)` (or simply `\(\sigma^2_p\)` for convenience). 

Adding this extra bit of information means we can do slightly better in trying to estimate the variance in the two groups. Our best guess for the pooled variance is 

`$$\hat{\sigma}^2_p = s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$$`

---

`$$\hat{\sigma}^2_p = s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$$`

Intuition: 

* this is a *weighted average* of our two best guesses

* we have a best guess for group 1, best guess for group 2, so surely the "truth" must be somewhere between the two.

* the group with more data (i.e. more information) gets more weight

--

* if the means in the two groups were the same, the pooled standard deviation is actually the same as just treating the two groups as one.
    - cannot do this when means are different because of definition of standard deviation

--

We now have that `\(\text{Var}(D) = \frac{s_p^2}{n_1} + \frac{s_p^2}{n_2} = s_p^2 (1/n_1 + 1/n_2)\)`, and our test statistic will follow a `\(t_{n_1+n_2-2}\)` distribution:

`$$T = \frac{D - d_0}{s_p\sqrt{1/n_1 + 1/n_2}} \sim t_{n_1 + n_2 - 2}$$`

---

**Scenario 2**: `\(\sigma_1^2 \neq \sigma_2^2\)`. 

In this case, we do not gain any insights, and so there's no adjustments we can make to the test statistic. 

It turns out that 

`$$T = \frac{D - d_0}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \sim t_{\nu},$$`

where 

$$
  \nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}
$$


--

In either case, we can find the distribution of `\(T\)`, and use this to either reject or not reject the null hypothesis!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ration": "16:10",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
