---
title: "Lecture 5+6: Random Variables"
subtitle: "STAT 324"
author: "Ralph Trane"
institute: "University of Wisconsin--Madison<br><br>"
date: "Spring 2020"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [../css/uwmadison.css, default-fonts, ../css/extra-classes.css]
    lib_dir: libs
    nature:
      titleSlideClass: [center, top, .title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ration: '16:9'
      navigation:
        scroll: false
---
layout: true

# Random Variables

```{r include = FALSE}
knitr::opts_chunk$set(dpi = 300, echo = FALSE, warning = FALSE, message = FALSE, fig.align = "center")
library(tidyverse); theme_set(theme_bw())
survey <- read_csv(here::here("csv_data/survey_cleaned.csv"))
```

---

**Why Random Variables?**

We introduce *random variables* to

* formalize the notion of an experiment

* simplify notation 

* have a rigorous way of discussing probabilities

All of this will be super helpful when talking about Statistical Hypothesis Testing later. 

---

**What is a "Random Variable"?**

A *random variable* is a variable tied to the outcome of an experiment. 

The value of it is unknown and uncertain before the experiment is conducted. 

Conducting the experiment results in a *realization* of the RV.

Distinguish between discrete and continuous RVs.

**Examples:**

1. $X = \text{flip of a coin}$. Possible outcomes:
--
 heads and tails. Discrete RV.

--

2. $X = \text{state of origin or randomly chosen student}$. Possible outcomes:
--
 Discrete RV.

--

3. $X = \text{age of randomly chosen student}$. Possible outcomes:
--
 Any value greater than $0$. Continuous RV.

--

4. $X = \text{height of randomly chosen student}$. Possible outcomes:
--
 any number greater than $0$. Continuous RV.

---
layout: true

# Discrete Random Variables

---

Talk about probabilities of different outcomes: 

1. $X = \text{flip of a coin}$. What is $P(X = \text{heads})$?

2. $X = \text{state of origin or randomly chosen student}$. What is $P(X \neq \text{Wisconsin})?$.

To calculate these probabilities, we need the *distribution* of the random variable. 

For a discrete RV, the distribution is a function that **specifies the probability of every possible outcome**.

---

**Example 1**

$X = \text{flip of a coin}$. The distribution of $X$ is given by

| x | P(X = x) |
|---|----------|
| 0 | 0.5 |
| 1 | 0.5 |

$P(X = x)$ is called the *probability mass function* (p.m.f.) of the random variable $X$.

---

**Example 2**

.pull-left[
$X =$ state of origin or randomly chosen student. The distribution is shown on the right.
]

.pull-right[
```{r echo = FALSE}
survey %>% 
  count(`What state did you grow up in?`) %>% 
  mutate(`P(X = x)` = n/sum(n)) %>% 
  select(-n) %>% 
  rename(`What state did you grow up in? (x)` = `What state did you grow up in?`) %>% 
  arrange(desc(`P(X = x)`)) %>% 
  knitr::kable(format = "html", digits = 3) %>% 
  kableExtra::kable_styling(font_size = 10)
```
]

---

**Example 3**

$X = \text{number of Packers fans in a sample of 3 students}$. 

Possible outcomes: 
--
 $0,1,2,3$. 

--
 
Distribution: need to specify $P(X = 0), P(X = 1), P(X = 2), P(X = 3)$. How can we do that?

--

Use math, or perform the experiment many, many, many times. Let's start with the latter.

---

```{r eval = FALSE, echo = FALSE}
packers_subset <- survey %>% 
  mutate(id = row_number()) %>% 
  select(id, packers = `Are you a Packers fan?`) %>% 
  filter(!is.na(packers))

packers_sample <- tibble(n_repeat = 1:100000) %>% 
  mutate(samples = map(n_repeat, function(x) sample_n(packers_subset, size = 3)),
         x = map_dbl(samples, function(x) sum(x$packers == "yes")))

theme_set(theme_bw())

write_rds(packers_sample,
          path = here::here('Lectures/lecture05-06/packers_sample.Rds'))

is <- c(1:9, 
        seq(10, 90, by = 10),
        seq(100, 900, by = 100),
        seq(1000, 9000, by = 1000),
        seq(10000, 100000, by = 10000))

for(i in is){
  i_format <- format(i, scientific = F)
  
  tmp <- packers_sample %>% 
    filter(row_number() <= i) %>% 
    janitor::tabyl(x) %>% 
    mutate(x = as.character(x)) %>% 
    ggplot(aes(x = x, y = percent)) +
      geom_bar(data = data.frame(x = as.character(0:3), y = c(0,0,0,0)),
               stat = 'identity',
               aes(x = x, y = y),
               alpha = 0) +
      geom_bar(stat = 'identity') + 
      geom_text(aes(label = n, y = 0.05),
                fontface = 'bold', size = 4) + 
      scale_y_continuous(limits = c(0,1), breaks = seq(0, 1, by = 0.1)) +
      labs(x = 'Number of Packers fans', y = 'proportion',
           title = paste0('Number of repetitions:\n', i_format)) + 
      theme(plot.title = element_text(size = 18, face = 'bold'))
  
  ggsave(tmp, filename = here::here('Lectures/lecture05-06/packers_figure', 
                                    paste0(str_pad(i_format, width = 6, side = 'left', pad = '0'), '0.jpg')),
         width = 4, height = 4)
  
  if(i <= 10)
    ggsave(tmp, filename = here::here('Lectures/lecture05-06/packers_figure', 
                                      paste0(str_pad(i_format, width = 6, side = 'left', pad = '0'), '1.jpg')),
         width = 4, height = 4)
}

system("ffmpeg -framerate 2 -pattern_type glob -i 'packers_figure/*.jpg' -c:v libx264 packers_anim.mp4 -y")
```

<center>
  <video controls width="450" height="550">
    <source src="packers_anim.mp4" type="video/mp4">
  </video>
</center>

---

From the sampling on the previous slide: 

```{r echo = FALSE}
packers_sample <- read_rds(here::here("Lectures/lecture05-06/packers_sample.rds"))
packers_sample %>% 
  janitor::tabyl(x) %>% 
  rename(proportion = percent) %>% 
  knitr::kable(format = "html") %>% 
  kableExtra::kable_styling(font_size = 15)
```

--
 
Math:

```{r echo = FALSE}
p_packers <- mean(survey$`Are you a Packers fan?` == 'yes', na.rm = T)

tibble(x = 0:3,
       `P(X = x)` = dbinom(x = x, prob = p_packers, size = 3) %>% round(digits = 3)) %>% 
  knitr::kable(format = "html") %>% 
  kableExtra::kable_styling(font_size = 15)
```

Notice how close they are.


---
layout: true

# Random Variables

---

**Properties**

Random variables have certain properties that are closely linked to the population that they represent.

1. Expected value 

2. Variance/SD

These are calculated slightly differently for discrete and continuous RVs. We will use the discrete case to gain some insights, and simply state similar results for continuous RVs.

---

**Expected Value**

The *expected value* of a discrete random variable $X$ is defined as follows:

$$
  E(X) = \sum_{i=1}^n P(X = x_i) \cdot x_i.
$$

Interpretation: this is the mean/average of the *entire population*.

---

**Variance**

The *variance* of a discrete random variable $X$ is defined as follows:

$$
  \text{Var}(X) = \sum_{i=1}^n P(X = x_i) \cdot (x_i - E(X))^2.
$$

Interpretation: this is the variance of the *entire population*.

---

In the following, $X$ is a random variable, and $c$ is a constant.

**Working with $E$**

* $E(c) =$ 
--
 $c$

--

* $E(c\cdot X) =$
--
 $c\cdot E(X)$

--

* $E(X + c) =$
--
 $E(X) + c$

--

* $E(X + Y) =$
--
 $E(X) + E(Y)$

---

**Working with $\text{Var}$**

* $\text{Var}(c) =$
--
 $0$

--

* $\text{Var}(c \cdot X) =$
--
 $c^2 \text{Var}(X)$

--

* $\text{Var}(X + c) =$
--
 $\text{Var}(X)$

--

* $\text{Var}(X + Y) =$
--
 $\text{Var}(X) + \text{Var}(Y)$ **ONLY IF INDEPENDENT!!!!**

--

Using bullets 2 and 4: if $X$ and $Y$ are independent, then 

* $\text{Var}(X - Y) =$
--
 $\text{Var}(X) + \text{Var}(Y)$.

---

**Bernoulli Distribution: definition and properties**

The *Bernoulli distribution* is a distribution with only two possible outcomes. When we say $X$ is a random variable following the *Bernoulli distribution*, this simply means that $X$ can only take on one of two values -- often denoted $1$ (success) and $0$ (failure). 

We also refer to this as a *Bernoulli trial*. 

The Bernoulli distribution has one parameter that we need to specify. We will denote this by $\pi$.

$\pi \in [0,1]$ is called the *probability of success*. 

Notation: $X \sim \text{Bernoulli}(p)$ simply means that $X$ can take two possible outcomes ( $0$ and $1$ ) and $P(X = 1) = p$. What is $P(X = 0)$? 

--

$P(X = 0) + P(X = 1) = 1$, so $P(X = 0) = 1 - \pi$.

What is $E(X)$ and $\text{Var}(X)$?

--

\begin{align}
  E(X) &= \pi \\
  \text{Var}(X) &= \pi\cdot (1-\pi).
\end{align}

---

**Bernoulli Distribution: Example**

Snapdragon plants have a gene that determines the presence of chlorophyll. The dominant allele (C) causes the plant to make chlorophyll, while the recessive allele (c) makes none. Snapdragons that are homozygous dominiant (CC) are green, whereas those that are heterozygous (Cc or cC) are yellow. The homozygous recessive plants die almost immediately due to lack of chlorophyll. Thus, a healthy adult snapdragon has either zero or one copy of the recessive allele. When two heterozygotic plants are crossed, the offspring have a 1/4 chance to be CC, with zero copies of the recessive, a 1/2 chance to be heterozygous, with one copy of the recessive, and a 1/4 chance to be cc and die. Therefore, if a healthy adult snapdragon is chosen at random from a large population of offspring of a heterozygous cross, about 1/3 will have zero copies of the recessive, and 2/3 will have one copy. 

$X =$ number of copies of the **recessive allele** for a randomly selected **healthy adult** snapdragon.

.pull-left[

Possible values of $X$? 

What is the pmf?

]

--

.pull-righ[

| x | P(X = x) |
|---|----------|
| 0 |    1/3   |
| 1 |    2/3   |

]

---

**Bernoulli Distribution: Example**

$X \sim \text{Bernoulli}(\pi = 2/3)$. What is $E(X)$ and $\text{Var}(X)?$

--

$$E(X) = \pi = 2/3$$ 

and 

$$\text{Var}(X) = \pi\cdot (1 - \pi) = 2/3 \cdot 1/3 = 2/9.$$

---

**Binomial Distribution: definition and properties**

The *Binomial* distribution with size $n$ and probability of success $\pi$ is the sum of $n$ independent Bernoulli trials with success parameter $\pi$.

I.e. if $X_1, X_2, ..., X_n \sim \text{Bernoulli}(\pi)$, then $Y = X_1 + X_2 + ... + X_n \sim \text{Binomial}(n, \pi)$.

The distribution of $Y$: $P(Y = k) = {n \choose k} \pi^{k} (1 - \pi)^{n-k}$. 

Illustration: app.

--

$$
E(Y) = n \cdot \pi
$$

and 

$$
\text{Var}(Y) = n\cdot \pi \cdot (1-\pi)
$$

Really enforces the idea that the binomial is $n$ Bernoulli's.

---

**Binomial Distribution: Example**

Consider the previous example, but say we sample $10$ healthy adult snapdragon plants independently. Let $X_1, X_2, ..., X_{10}$ denote the number of copies of the recessive allele for the $10$ plants.

Then, $X_i \sim \text{Bernoulli}(\pi = 2/3)$. Since these are independent, the *total number of plants with the recessive allele* $Y$ follow a Binomial distribution with $n=10$ and $\pi = 2/3$.

$Y = X_1 + X_2 + ... + X_{10} \sim \text{Binomial}(10, 2/3)$.

$E(Y) =$
--
 $10 \cdot \frac{2}{3} = \frac{20}{3} \approx 6.67$.

--

$\text{Var}(Y) =$
--
 $10 \cdot \frac{2}{3} \frac{1}{9} = \frac{20}{9} \approx 2.22$.

---
layout: true

# Continuous Random Variables

---

For a continuous variable, can we specify the probability of every single possible outcome?
--
 No, because number of outcomes is uncountable!

Instead, define a curve. 

---

```{r echo = F}
library(distributions3)
library(patchwork)
X <- Normal(mu = 170, sigma = 15)
set.seed(11100110)
X_samples <- tibble(x = random(X, n = 1000000)) %>% mutate(i = row_number())
```


Observe the height of $10$ individuals, draw a histogram with $10$ bins.

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram <- function(I = 10, bins = 10){
  ggplot(data = X_samples %>% filter(i <= I),
         aes(x = x)) + 
    geom_histogram(bins = bins, boundary = 0,
                   aes(y = ..density..)) + 
    scale_x_continuous(limits = c(90, 240),
                       breaks = seq(90, 240, by = 10))
}

normal_histogram()
```

---

Observe the height of $100$ individuals, draw a histogram with $20$ bins.

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = 100, bins = 20)
```

---

Observe the height of $1000$ individuals, draw a histogram with $75$ bins.

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = 1000, bins = 75)
```

---

Observe the height of $10000$ individuals, draw a histogram with $100$ bins

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = 10000, bins = 100)
```

---

Observe the height of $100000$ individuals, draw a histogram with $125$ bins.

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = 100000, bins = 125)
```

---

Observe the height of $1000000$ individuals, and $150$ bins. 

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = 1000000, bins = 150)
```

---

The data here was simulated from a *normal distribution* with mean `r X[['mu']]` and variance `r X[['sigma']]^2` (more on this in a second). This distribution looks like this:

```{r fig.width=5, fig.height = 5, out.width = 400}
ggplot(data = NULL, 
       aes(x = seq(X$mu - 3*X$sigma, X$mu + 3*X$sigma, by = X$sigma/100))) + 
  stat_function(fun = dnorm, args = list(mean = X$mu, sd = X$sigma),
                color = 'red', size = 1) +
  labs(x = 'x', y = 'density')
```

---

If we overlay it:

```{r fig.width=5, fig.height = 5, out.width = 400}
normal_histogram(I = Inf, bins = 150) +
  stat_function(fun = dnorm, args = list(mean = X$mu, sd = X$sigma),
                color = 'red', size = 1)
```

In words: the distribution of a continuous RV is the curve that appears when a histogram with narrow bars of many, many, many observations is drawn. 

---

```{r out.width = "100%", out.height = "62.5%", fig.height = 5, fig.width = 8}
{
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x <= 185, y, 0)),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("") +
    guides(fill = "none") +
    geom_label(data = data.frame(x = 136, y = 0.025, label = "P(X < 185)"),
              aes(label = label), size = 3) +
    theme_bw()
} + {
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x >= 172, y, 0)),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("", begin = 1) +
    guides(fill = "none") +
    geom_label(data = data.frame(x = 136, y = 0.025, label = "P(X > 172)"),
              aes(label = label), size = 3) +
    theme_bw()
} + {
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x <= 185 & x >= 172, y, 0)),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("") +
    guides(fill = "none") +
    geom_label(data = data.frame(x = 138.5, y = 0.025, label = "P(172 < X < 185)"),
              aes(label = label), size = 3) +
    theme_bw()
} + {
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x >= 185 | x <= 162, y, 0)),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("", begin = 1) +
    guides(fill = "none") +
    geom_label(data = data.frame(x = 140, y = 0.025, label = "P(X > 185 or X < 162)"),
              aes(label = label), size = 3) +
    theme_bw()
} + plot_layout(ncol = 2, nrow = 2)
```

---
 
Question: If $X$ is a continuous RV, what is $P(X = x)$? 

--

No matter what $x$ you pick, and no matter what (continuous) distribution $X$ follows, $P(X = x) = 0$!!!

This also means that 

$$
P(X \le x) = P(X < x) + P(X = x) = P(X < x).
$$

---

In the following, $X$ and $Y$ are RVs, and $c$ is a constant.

.pull-left[
**Working with $E$**

* $E(c) = c$

* $E(c\cdot X) = c\cdot E(X)$

* $E(X + c) = E(X) + c$

* $E(X + Y) = E(X) + E(Y)$
]

.pull-right[
**Working with $\text{Var}$**

* $\text{Var}(c) = 0$

* $\text{Var}(c \cdot X) = c^2 \text{Var}(X)$

* $\text{Var}(X + c) = \text{Var}(X)$

* $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$ **ONLY IF INDEPENDENT!!!!**

Using bullets 2 and 4: if $X$ and $Y$ are independent, then 

* $\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y)$.
]

---

**Normal Distribution**

The Normal Distribution (also known as the Gaussian Distribution) is a continuous distribution.

It is specified using two parameters: the mean $\mu$, and the variance $\sigma^2$. If $X$ follows a normal distribution with mean $\mu$, and variance $\sigma^2$, we write $X \sim N(\mu, \sigma^2)$ (or $X \sim \text{Normal}(\mu, \sigma^2)$).

The curve is given by the function 

$$f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).$$

$E(X) = \mu$ and $\text{Var}(X) = \sigma^2$. 

---

**Normal Distribution**

```{r out.width = 500, fig.width = 4, fig.height = 3, fig.align = "center"}
ggplot(data = tibble(x = seq(-5, 8, by = 0.01)),
       aes(x = x)) + 
  stat_function(fun = dnorm,
                aes(color = 'N(2,4)'),
                args = list(mean = 2, sd = 2)) + 
  stat_function(fun = dnorm,
                aes(color = 'N(-2,1)'),
                args = list(mean = -2, sd = 1)) +
  stat_function(fun = dnorm,
                aes(color = 'N(0,5)'),
                args = list(mean = 0, sd = sqrt(5))) +
  scale_color_discrete("")
```

---

**Normal Distribution: properties**

Let's say that $X \sim N(\mu, \sigma^2)$. Then 

* $X + c$ also follows a Normal distribution. Specifically, 
--
 $X + c \sim N(\mu + c, \sigma^2)$
--

* $c\cdot X$ also follows a Normal distribution. Specifically,
--
 $c\cdot X \sim N(c\cdot \mu, c^2 \sigma^2)$

--

If $Y \sim N(\mu_Y, \sigma_Y^2)$, then $X + Y$ is also a normally distributed RV. Specifically, (if $X$ and $Y$ are independent)
--

$$
X + Y \sim N(\mu + \mu_Y, \sigma^2 + \sigma_Y^2).
$$

---
layout: true

# Continuous Random Variables

**Probabilities from a curve**

---

Probability = area under the curve. $X \sim N(`r X[["mu"]]`, `r X[["sigma"]]`^2)$

.pull-left[

What is $P(X \le 160)$?

It is the shaded area on the following figure

```{r out.width = 450, fig.width = 4, fig.height = 3}
ggplot(data = NULL, 
       aes(x = seq(X$mu - 3*X$sigma, X$mu + 3*X$sigma, by = X$sigma/100))) + 
  stat_function(fun = dnorm, args = list(mean = X$mu, sd = X$sigma),
                size = 1) + 
  geom_area(aes(x = seq(X$mu - 3*X$sigma, 160, by = 0.1),
                y = dnorm(seq(X$mu - 3*X$sigma, 160, by = 0.1), mean = X$mu, sd = X$sigma)),
            alpha = 0.75) +
  labs(x = 'x', y = 'Density')
```
]

--

.pull-right[
.vsmall[
```{r echo = T}
library(distributions3)

X <- Normal(mu = 170, sigma = 15)

cdf(X, 160)
```
]
]

---

.pull-left[
.small[
Probability = area under the curve. 

What is $P(172 < X < 185)$?

It is the shaded area on the following figure
]
```{r out.width = 450, fig.width = 4, fig.height = 3}
ggplot(data = NULL, 
       aes(x = seq(X$mu - 3*X$sigma, X$mu + 3*X$sigma, by = X$sigma/100))) + 
  stat_function(fun = dnorm, args = list(mean = X$mu, sd = X$sigma),
                size = 1) + 
  geom_area(aes(x = seq(172, 185, by = 0.1),
                y = dnorm(seq(172, 185, by = 0.1), mean = X$mu, sd = X$sigma)),
            alpha = 0.75) +
  labs(x = 'x', y = 'Density')
```

.vsmall[
```{r  echo = T}
cdf(X, 185) - cdf(X, 172)
```
]

]

--

.pull-right[

.vsmall[
```{r echo = T}
cdf(X, 185)
```


```{r out.width = 450, fig.width = 4, fig.height = 4, fig.align = "center"}
{
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x <= 185, y, 0),
                  fill = '<185'),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("") +
    guides(fill = "none") +
    theme_bw()
} / {
  plot_pdf(X) + 
    geom_area(aes(y = if_else(x <= 172, y, 0),
                  fill = '<172'),
              alpha = 0.3) +
    labs(y = "Density") +
    scale_fill_viridis_d("", begin = 1) +
    guides(fill = "none") +
    theme_bw()
}
```


```{r echo = T}
cdf(X, 172)
```
]
]

---
layout: true

# Continuous Random Variables

---

Revisit $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$ **ONLY IF INDEPENDENT**.

Let's just take a moment to think about especially the last point.

Here are two data sets. Each has two variables. In one data set the variables are independent, in the other they are not. 

```{r echo = FALSE, fig.width = 6, fig.height = 2.5, out.width = "800px", out.height = "333px"}
dep_vars <- MASS::mvrnorm(n = 1000, 
                          mu = c(1, 5), 
                          Sigma = matrix(c(1, -0.9, -0.9, 1), 
                                         nrow = 2, byrow = T)) %>% 
  as_tibble() %>% 
  setNames(c("X", "Y")) %>% 
  mutate(type = 'dependent')

ind_vars <- tibble(X = rnorm(1000, 1, 1),
                   Y = rnorm(1000, 5, 1),
                   type = 'independent')

both <- bind_rows(dep_vars, ind_vars)

ggplot(data = both, aes(x = X, y = Y)) +
  geom_point(alpha = 0.5) +  
  facet_grid(type ~ .)
```

---


What does $X + Y$ mean? It means "perform experiment $X$, perform experiment $Y$, then add the outcomes." 

We do that, and look at the distributions of the results (i.e. histograms):

--

```{r echo = FALSE, fig.width = 6, fig.height = 3, out.width = "800px", out.height = "400px"}
both %>% 
  mutate(`X + Y` = X + Y) %>% 
  pivot_longer(cols = -type, 
               names_to = 'vars', values_to = 'value') %>% 
  ggplot(aes(x = value)) + 
    geom_histogram(bins = 40) + 
    facet_grid(type ~ vars)
```

---
layout: true

# Continuous Random Variables
