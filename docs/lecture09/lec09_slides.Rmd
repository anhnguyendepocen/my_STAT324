---
title: "Lecture 9: Central Limit Theorem"
subtitle: "STAT 324"
author: "Ralph Trane"
institute: "University of Wisconsin--Madison<br><br>"
date: "Spring 2020"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [../css/uwmadison.css, default-fonts, ../css/extra-classes.css]
    lib_dir: libs
    nature:
      titleSlideClass: [center, top, .title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ration: '16:9'
      navigation:
        scroll: false
---

```{r echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      dpi = 300, out.width = "400px", out.height = "400px", 
                      fig.width = 4, fig.height = 4, fig.align = "center")

library(tidyverse)
library(distributions3)
library(patchwork)
```


# Previously on STAT 324...

---
layout: true

# Confidence Intervals

---

**Scenario 1:**

The data (i.e. $X_1, ..., X_n$) are normally distributed, independent, and **we know** $\sigma$. Then

$$
  \bar{X} \sim N(\mu, \sigma^2/n) \qquad \text{and} \qquad \frac{\bar{X} -\mu}{\sigma/\sqrt{n}} \sim N(0,1),
$$

so 

$$P\left(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha$$

where $z_{\alpha/2}$ is the number such that $P(Z > z_{\alpha/2}) = \alpha/2$

```{r fig.height = 2.25, out.height = "225px", out.width = "400px"}
plot_pdf(Normal()) + 
  geom_auc(from = 1.96, fill = "grey80") +
  geom_vline(aes(xintercept = 1.96), linetype = "dashed",
             color = "red") + 
  # scale_color_manual("", values = c("black"), 
  #                    labels = c("N(0,1)")) + 
  annotate(x = 2.5, y = 0.1, geom = 'text', size = 7,
           label = expression(alpha/2)) +
  geom_segment(data = data.frame(x = 2.4, y = 0.075, xend = 2.2, yend = 0.025),
               aes(x = x, y = y, xend = xend, yend = yend),
               color = "black",
               arrow = arrow(length = unit(0.1, "in"))) +
  guides(fill = "none") +
  scale_x_continuous(sec.axis = sec_axis(~., breaks = 1.96, labels = expression(z[alpha/2]))) +
  labs(title = "Curve of N(0,1)",
       x = "", y = "Density") +
  theme(legend.position = "top",
        title = element_text(size = 18),
        axis.text.x.top = element_text(size = 14))
```


---

**Scenario 2:**

The data (i.e. $X_1, ..., X_n$) are normally distributed, independent, but we **do not** know $\sigma$. Then

$$
  \bar{X} \sim N(\mu, \sigma^2/n) \qquad \text{and} \qquad \frac{\bar{X} -\mu}{s/\sqrt{n}} \sim t_{n-1},
$$

so 

$$P\left(\bar{X} - t_{n-1,\alpha/2}\frac{s}{\sqrt{n}} < \mu < \bar{X} + t_{n-1,\alpha/2}\frac{s}{\sqrt{n}}\right) = 1 - \alpha$$

where $t_{n-1,\alpha/2}$ is the number such that $P(T_{n-1} > t_{n-1,\alpha/2}) = \alpha/2$

```{r fig.height = 2.25, out.height = "225px", out.width = "400px"}
T <- StudentsT(df = 3)

plot_pdf(T, p = 0.01) + 
  geom_auc(from = quantile(T, 0.975), fill = "grey80") +
  geom_vline(aes(xintercept = quantile(T, 0.975)), linetype = "dashed",
             color = "red") + 
  annotate(x = 4, y = 0.1, geom = 'text', size = 7,
           label = expression(alpha/2)) +
  geom_segment(data = data.frame(x = 4, y = 0.075, xend = 3.5, yend = 0.02),
               aes(x = x, y = y, xend = xend, yend = yend),
               color = "black",
               arrow = arrow(length = unit(0.1, "in"))) +
  guides(fill = "none") +
  scale_x_continuous(sec.axis = sec_axis(~., breaks = quantile(T, 0.975), 
                                         labels = expression(t[n-1~","~alpha/2]))) +
  labs(title = expression("Curve of"~t[n-1]),
       x = "", y = "Density") +
  theme(legend.position = "top",
        title = element_text(size = 18),
        axis.text.x.top = element_text(size = 14))
```

---

Recall: paint thickness data. 

```{r echo = T}
library(tidyverse); library(distributions3)
paint_thickness <- tibble(
  thickness = c(1.29, 1.12, 0.88, 1.65, 1.48, 1.59, 1.04, 0.83, 
                1.76, 1.31, 0.88, 1.71, 1.83, 1.09, 1.62, 1.49))
```

Create $95\%$ confidence interval WITHOUT assuming we know the true SD. Need mean, sd, and critical value. 

Mean and SD:

```{r echo = T}
paint_thickness %>% 
  summarize(mean = mean(thickness), sd = sd(thickness))
```

--

Critical value (recall, df = n - 1):

.pull-left[
```{r tcrit, eval = F, echo = T}
T <- StudentsT(df = 16-1) 

(t_crit <- quantile(T, 0.975))
```
]

.pull-right[
```{r ref.label="tcrit", eval = T, echo = F}
```

]
---

We can find the confidence interval as 

$$
  \bar{x} \pm t_{15, 0.025} \cdot \frac{s}{\sqrt{16}}
$$

```{r echo = T}
paint_thickness %>% 
  summarize(mean = mean(thickness), 
            sd = sd(thickness),
            LL = mean - t_crit*sd/sqrt(n()), 
            UL = mean + t_crit*sd/sqrt(n()))
```

---

**Some vocabulary and intuition**

Our *estimate* of the true mean paint thickness is $1.348$ - also call this the *point estimate*. 

The interval $1.168$ to $1.529$ is a $95\%$ confidence interval - also call this an *interval estimate*. 

We are $95\%$ *confident* the true paint thickness is between $1.168$ and $1.529$.

Compare to $95\%$ CI when knowing SD: $1.182$ to $1.515$. When SD unknown, CI larger. 

Intuitively, we know less, so less confident. 

---

What if we do not know if $X_1, ..., X_n$ are normally distributed? 

Or maybe we know they are in fact NOT normally distributed. Then what? 

--

**Central Limit Theorem**

If $X_1, ..., X_n$ are iid random variables with $E(X_i) = \mu$ and $\text{Var}(X_i) = \sigma^2$. For "large enough" $n$, 

$$
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right) \quad (\text{approximately})
$$

$n$ "large enough" depends on the true distribution of $X_i$'s. If "close to normal", smaller $n$ needed. 

Generally, $n \ge 30$ is a good rule of thumb for "large enough".

--

I, personally, find it easier to remember that $\bar{X} \sim N(E(\bar{X}), \text{Var}(\bar{X}))$. (Note: this is the same, since $E(\bar{X}) = \mu$ and $\text{Var}(\bar{X}) = \sigma^2/n$.)

---
layout: true

# Confidence Intervals: Population Proportion

---

* An accounting firm has a large list of clients, each client has a file with information. 
* Noticed some files contain errors
* What proportion of all files contain errors?

Parameter of interest: $\pi$ = true proportion of files containing errors.

Don't want to go through all files, so take a simple random sample of size $50$. Let $X_1, ..., X_{50}$ be the random variables denoting if the files have an error or not. If file number $i$ has an error, $X_i = 1$. Otherwise, $X_i = 0$. 

--

Distribution of $X_i$ is 
--
 $\text{Bernoulli}(\pi)$. 

--

A good estimator of the true proportion of files with errors is
--
 $P = \frac{\sum_{i=1}^{50} X_i}{n}$ the sample proportion. 

---

$E(P) =$
--
 $\pi$

--

$\text{Var}(P) =$
--
 $\frac{\pi(1-\pi)}{n}$

--

To find a $CI$ for $\pi$, we need the distribution of $P$. Since $P = \frac{1}{n} \sum_{i=1}^{50} X_i$, $P$ is an average,
--
 CLT tells us that, for $n$ large enough, $P \sim N(E(P), \text{Var}(P))$, or equivalently $P \sim N(\pi, \pi(1-\pi)/n)$.
 
--

Since we do not know $\text{Var}(P)$, we will use $\widehat{\text{SD}}(P) = \sqrt{P(1-P)/n}$. 

---

$P \sim N(\pi, \pi(1-\pi)/n)$. So what is the distribution of $\frac{P - \pi}{\sqrt{P(1-P)/n}}$? 

--

$$
  \frac{P - \pi}{\sqrt{P(1-P)/n}} = \frac{P - E(P)}{\widehat{\text{SD}}(P)} = Z \sim N(0,1).
$$

Why not $t$? 
--
 Because estimating $\pi$ with $P$ gives us $\widehat{SD}(P)$ for free! No extra estimation required. 

.pull-left[

Now, we can find values $x_1, x_2$ such that $P(Z \le x_1) + P(Z \ge x_2) = \alpha$. Let's for simplicity use $\alpha = 0.05$. I.e. we want to find $x_1,x_2$ such that this area is $0.05$.

If we decide the two areas in the tails are the same, $x_1 = -x_2$. 

$x_2$ is by definition the $\alpha/2$ (0.025 in this case) critical value, $z_{\alpha/2}$ - it cuts off $\alpha/2$ (0.025) to the right!

]

.pull-right[

```{r echo =FALSE, out.width = "300px", out.height = "300px"}
Z <- Normal()
plot_pdf(Z) + 
  geom_auc(to = -1.96) +
  geom_auc(from = 1.96) +
  geom_vline(data = data.frame(),
             aes(xintercept = c(-1.96, 1.96)), color = "red", linetype = "dashed") +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1), add = c(0.001, 0)))
```

]

---

So, 

$$
\begin{aligned}
  1-\alpha &= P(-z_{\alpha/2} \le Z \le z_{\alpha/2}) \\
           & \\
           &= P\left(-z_{\alpha/2} \le \frac{P - \pi}{\sqrt{P(1-P)/n}} \le z_{\alpha/2}\right) \\
           & \\
           &= P\left(-z_{\alpha/2}\sqrt{P(1-P)/n} \le P - \pi \le z_{\alpha/2}\sqrt{P(1-P)/n}\right) \\
           & \\
           &= P\left(-P - z_{\alpha/2}\sqrt{P(1-P)/n} \le - \pi \le - P + z_{\alpha/2}\sqrt{P(1-P)/n}\right) \\
           & \\
           &= P\left(P + z_{\alpha/2}\sqrt{P(1-P)/n} \ge \pi \ge P - z_{\alpha/2}\sqrt{P(1-P)/n}\right) \\
           & \\
           &= P\left(P - z_{\alpha/2}\sqrt{P(1-P)/n} \le \pi \le P + z_{\alpha/2}\sqrt{P(1-P)/n}\right)
\end{aligned}
$$

---

So, a $(1-\alpha)\cdot 100\%$ Confidence Interval for the true population proportion $\pi$ is $[P - z_{\alpha/2}\sqrt{P(1-P)/n}, P + z_{\alpha/2}\sqrt{P(1-P)/n}]$. 

---

Say we observe $10$ files with errors and $40$ files without. 

Our estimate would be $p =$
--
 $\frac{1}{50}\sum_{i=1}^n x_i = \frac{1}{50}(10\cdot 1 + 40 \cdot 0) = 0.2$. 

The estimated SD would be $\sqrt{p(1-p)/n} = \sqrt{0.2\cdot 0.8/50} \approx `r round(sqrt(0.2*0.8/50), digits = 3)`$.

--

So a $95\%$ CI for the true population proportion has lower limit 

$$
  p - z_{\alpha/2}\widehat{\text{SD}}(P) = 0.2 - 1.96 \cdot `r round(sqrt(0.2*0.8/50), digits = 3)` = `r round(0.2 - 1.96 * sqrt(0.2*0.8/50), digits = 3)`
$$

and upper limit

$$
  p + z_{\alpha/2}\widehat{\text{SD}}(P) = 0.2 + 1.96 \cdot `r round(sqrt(0.2*0.8/50), digits = 3)` = `r round(0.2 + 1.96 * sqrt(0.2*0.8/50), digits = 3)`
$$


---

There's a pretty strong pattern here: if $\bar{X}$ is normally distributed, then a $(1-\alpha)\cdot 100\%$ CI for the true value of $E(\bar{X})$ (which is also the true value of $E(X_i)$) is 

* $\bar{X} \pm z_{\alpha/2} \widehat{\text{SD}}(\bar{X})$ if calculating $\bar{X}$ gives us $\widehat{\text{SD}}(\bar{X})$ "for free",

* $\bar{X} \pm t_{\alpha/2} \widehat{\text{SD}}(\bar{X})$ if we still need to estimate $\widehat{\text{SD}}(\bar{X})$.

This "average $\pm$ critical value $\times$ standard deviation" pattern comes up all the time. 

---
layout: true

# Confidence Interval: CLT Examples

**Do you like snow?**

---

.pull-left[
True distribution:

```{r echo = FALSE}
q <- quo(`Do you like snow?`)

student_survey <- read_csv(here::here("csv_data/survey_cleaned.csv")) %>% 
  filter(!is.na(!!q)) %>% 
  mutate(!!q := !!q == "yes") %>% 
  mutate_at(vars(!!q), as.numeric)
  # filter(!!q <= 40)
  # filter(!!q <= 3*IQR(!!q) + stats::quantile(!!q, p = 0.75))

theme_set(theme_bw())

ggplot(student_survey,
       aes(x = !!q)) + 
  geom_histogram(binwidth = 0.5, boundary = 0)
```
]

.pull-right[

Not normal because:

* not symmetrical 

* not even continuous!!!

]

---

Say we didn't have the entire population data. Just a sample of 20 students:

```{r}
true_mean <- student_survey %>% pull(!!q) %>% mean()
true_sd <- true_mean*(1-true_mean)

sample_size <- 20

resamples <- tibble(i = 1:5000) %>% 
  mutate(sample = map(i, ~sample_n(student_survey, size = sample_size) %>% pull(!!q)),
         mean = map_dbl(sample, mean),
         sd = map_dbl(sample, sd),
         #t_stat = (mean - true_mean)/(sd/sqrt(sample_size)),
         LL = mean - 1.96*sqrt(mean*(1-mean)/sample_size),
         UL = mean + 1.96*sqrt(mean*(1-mean)/sample_size),
         contains_truth = LL < true_mean & UL > true_mean)
```

$$
`r paste(resamples[["sample"]][[1]], collapse = ", ")`
$$

Estimated propotion: $p = \frac{`r sum(resamples[["sample"]][[1]])`}{`r sample_size`} = `r (p <- round(mean(resamples[["sample"]][[1]]), digits = 3))`$. 

Estimated standard deviation of $P$: $\widehat{\text{SD}}(P) = \sqrt{p(1-p)/n} = \sqrt{`r p`\cdot `r 1-p`/`r sample_size`} = `r (est_sd <- round(sqrt(p*(1-p)/sample_size), digits = 3))`$.

So a $95\%$ CI is $`r p` \pm 1.96 \cdot `r est_sd` = [`r paste(round(p + c(-1,1)*1.96*est_sd, digits = 3), collapse = ", ")`]$.

--

For once, we know the truth: `r round(true_mean, digits = 3)`. 

---

Let's repeat the process many, many times. Actually, I redo this 5000 times!

```{r}
ggplot(data = resamples,
       aes(x = mean)) + 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.05) +
  stat_function(fun = pdf, args = list(d = Normal(mu = true_mean, sigma = sqrt(true_mean*(1-true_mean)/sample_size))))
```

---

If the confidence interval correct, it should contain the true value 95% of the time. Here are the first 100:

```{r out.width = "500px", fig.width = 5, fig.height = 3, out.height = "300px"}
ggplot(resamples %>% filter(i <= 100),
       aes(x = 2*(i-1)+1, y = mean, ymin = LL, ymax = UL, color = contains_truth)) +
  scale_color_manual("Contains the true value", values = c("red", "black"), labels = c("No", "Yes")) +
  scale_x_continuous("", breaks = NULL) +
  geom_errorbar() +
  geom_point() + 
  geom_hline(yintercept = true_mean, color = "red", linetype = "dashed") +
  theme(legend.position = "top")
```

Proportion of all 5000 CIs containing the true value: `r round(mean(resamples$contains_truth), digits = 3)`. Pretty good!



---
layout: true

# Confidence Interval: CLT Examples

**What do you think of math?**

---

.pull-left[

True distribution:

```{r echo = FALSE, fig.width = 6, out.width = "600px"}
q <- quo(`What do you think of math?`)

student_survey <- read_csv(here::here("csv_data/survey_cleaned.csv")) %>% 
  filter(!is.na(!!q)) 
  # filter(!!q <= 40)
  # filter(!!q <= 3*IQR(!!q) + stats::quantile(!!q, p = 0.75))

theme_set(theme_bw())

{
  ggplot(student_survey,
         aes(x = !!q)) + 
    geom_histogram(binwidth = 1, boundary = 0)
} + {
  ggplot(student_survey,
         aes(sample = !!q)) + 
    geom_qq() + 
    geom_abline(aes(slope = sd(!!q), intercept = mean(!!q)))
} + plot_layout(ncol = 1)
```

]

.pull-right[

Not normal because:

* not symmetrical (left skewed)

* not even continuous!!!
]

---

Say we didn't have the entire population data. Just a sample of 20 students:

```{r}
true_mean <- student_survey %>% pull(!!q) %>% mean()
true_sd <- student_survey %>% pull(!!q) %>% sd()

sample_size <- 20

resamples <- tibble(i = 1:5000) %>% 
  mutate(sample = map(i, ~sample_n(student_survey, size = sample_size) %>% pull(!!q)),
         mean = map_dbl(sample, mean),
         sd = map_dbl(sample, sd),
         #t_stat = (mean - true_mean)/(sd/sqrt(sample_size)),
         LL = mean - 1.96*sd/sqrt(sample_size),
         UL = mean + 1.96*sd/sqrt(sample_size),
         contains_truth = LL < true_mean & UL > true_mean)
```

$$
`r paste(resamples[["sample"]][[1]], collapse = ", ")`
$$

Estimated mean: $\bar{x} = `r (xbar <- resamples[["mean"]][1])`$. 

Estimated standard deviation of $\bar{X}$: $\widehat{\text{SD}}(\bar{X}) = `r (est_sd <- round(resamples[["sd"]][1], digits = 3)/sqrt(sample_size))`$.

So a $95\%$ CI is 

$$
\begin{aligned}
\bar{x} \pm t_{`r sample_size-1`, 0.025} s/\sqrt{`r sample_size`} &= `r xbar` \pm `r (t_crit <- qt(df = sample_size - 1, p = 0.975))` \cdot `r est_sd` \\
                                                                             &= [`r paste(round(xbar + c(-1,1)*1.96*est_sd, digits = 3), collapse = ", ")`]
\end{aligned}
$$

--

For once, we know the truth: `r round(true_mean, digits = 3)`. 

---

Let's repeat the process many, many times. Actually, I redo this 5000 times!

```{r}
ggplot(data = resamples,
       aes(x = mean)) + 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.05) +
  stat_function(fun = pdf, args = list(d = Normal(mu = true_mean, sigma = true_sd/sqrt(sample_size))))
```

---

If the confidence interval correct, it should contain the true value 95% of the time. Here are the first 100:

```{r out.width = "500px", fig.width = 5, fig.height = 3, out.height = "300px"}
ggplot(resamples %>% filter(i <= 100),
       aes(x = 2*(i-1)+1, y = mean, ymin = LL, ymax = UL, color = contains_truth)) +
  scale_x_continuous("", breaks = NULL) +
  scale_color_manual("Contains the true value", values = c("red", "black"), labels = c("No", "Yes")) +
  geom_errorbar() +
  geom_point() + 
  geom_hline(yintercept = true_mean, color = "red", linetype = "dashed") +
  theme(legend.position = "top")
```

Proportion of all 5000 CIs containing the true value: `r round(mean(resamples$contains_truth), digits = 3)`. Pretty good!

---
layout: false

# Confidence Intervals: Summary I

It is all about finding an estimator for parameter of interest, and finding the distribution of that estimator. To find the distribution, the Central Limit Theorem is a powerful ally.

* If data are from a normal distribution and $\sigma$ known: $\bar{X} \sim N$ and $\bar{X} \pm z_{\alpha/2}\text{SD}(\bar{X})$ contains the true value of $E(X_i)$ $(1-\alpha)\cdot 100\%$ of the time
    * $\text{SD}(\bar{X}) = \sigma/\sqrt{n}$
    
* If data are from a normal distribution and $\sigma$ unknown: $\bar{X} \sim N$ and $\bar{X} \pm t_{n-1,\alpha/2}\widehat{\text{SD}}(\bar{X})$ contains the true value of $E(X_i)$ $(1-\alpha)\cdot 100\%$ of the time
    * $\widehat{\text{SD}}(\bar{X}) = S/\sqrt{n} = \frac{\sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2}}{\sqrt{n}}$

---

# Confidence Intervals: Summary II

* If data are binary, and $n$ "large enough": $P \sim N$ and $P \pm z_{\alpha/2}\widehat{\text{SD}}(P)$ contains the true value of $E(X_i)$ $(1-\alpha)\cdot 100\%$ of the time
    * $\widehat{\text{SD}}(P) = \sqrt{P(1-P)/n}$
    * $n$ is large enough if $n\cdot \pi > 5$ and $n\cdot (1-\pi) > 5$. 
    * We check this using the estimated value of $\pi$, i.e. $p$. So we check if $n\cdot p > 5$ and $n\cdot (1-p) > 5$.

* If data are NOT from a normal distribution, and $n$ "large enough": $\bar{X} \sim N$ and $\bar{X} \pm t_{n-1,\alpha/2}\widehat{\text{SD}}(\bar{X})$ contains the true value of $E(X_i)$ $(1-\alpha)\cdot 100\%$ of the time
    * $\widehat{\text{SD}}(\bar{X}) = S/\sqrt{n} = \frac{\sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2}}{\sqrt{n}}$
    * usually, $n \ge 30$ satisfies $n$ "large enough"
