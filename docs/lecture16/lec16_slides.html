<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 16: Two Sample Hypothesis Tests</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ralph Trane" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="libs/jquery-1.12.4/jquery.min.js"></script>
    <link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding-0.9/datatables.js"></script>
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core-1.10.19/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../css/uwmadison.css" type="text/css" />
    <link rel="stylesheet" href="../css/extra-classes.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, .title-slide, title-slide

# Lecture 16: Two Sample Hypothesis Tests
## STAT 324
### Ralph Trane
### University of Wisconsinâ€“Madison<br><br>
### Spring 2020

---

layout: true

# Two Independent Samples Hypothesis Test

---





**Last time**

Two sample T-tests. Objective: test the hypothesis `\(H_0: \mu_1 - \mu_2 = v_0\)` against an alternative. (We only considered `\(v_0 = 0\)`, but could be any number. We only considered `\(H_A: \mu_1 - \mu_2 \neq v_0\)`, but could be any of the three `\(&gt;, &lt;, \neq\)`.)

Assumptions: 
  
  * the two samples are independent of each other
  
  * the observations in each sample are independent
  
  * and the averages `\(\bar{X}_1\)` and `\(\bar{X}_2\)` are normally distributed.

Test statistic: `\(T = \frac{V - v_0}{\widehat{\text{SD}}(V)}\)` which follows a `\(t\)`-distribution **IF** the null hypothesis is true.

---

**Last time**

Two scenarios determines how we calculate `\(\widehat{\text{SD}}(V)\)`, and the degrees of freedom for the `\(t\)`-distribution:

.pull-left[
If `\(0.5 &lt; \frac{s_1}{s_2} &lt; 2\)`, we assume equal variances `\(\sigma_1^2 = \sigma_2^2\)`. In this case, 

* `\(\widehat{\text{SD}}(V) = s_p \sqrt{1/n_1 + 1/n_2}\)`, where `$$s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}$$`

* if `\(H_0\)` is true, `\(T \sim t_{n_1 + n_2 - 2}\)`. 
]

.pull-right[
If we cannot assume equal variances,

* `\(\widehat{\text{SD}}(V) = \sqrt{s_1^2/n_1 + s_2^2/n_2}\)`

* if `\(H_0\)` is true, `\(T \sim t_{\nu}\)` where `$$\nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}$$`
]


---

**Example: Should we be drinking Turkish wine?** 

We all know that France, but what if I told you that Turkish wine is almost as good, at a fraction of the price?!

Some data from [Kaggle](https://www.kaggle.com/zynicide/wine-reviews). Here are top 15 countries in terms of mean point scores:

&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-2-1.png" height="400px" style="display: block; margin: auto;" /&gt;

---

Let's look at some summaries for the top 15:

.pull-left[

```r
wine %&gt;% 
  filter(!is.na(country)) %&gt;% 
  group_by(country) %&gt;% 
  summarize(Points = mean(points, na.rm = T),
            Price = mean(price, na.rm = T),
            n = n()) %&gt;% 
  top_n(15, Points) %&gt;% 
  mutate_at(.vars = vars(Points, Price),
            round, digits = 2) %&gt;%
  arrange(desc(Points)) %&gt;% 
  DT::datatable(options = list(pageLength = 10, digits = 2, dom = "tip"), 
                class = "cell-border stripe")
```

England: only 9 observations. 

Austria: basically same score as France.

Morocco: only 12 observations.

Turkey: cheaper with good `\(n\)`
]

.pull-right[
<div id="htmlwidget-4c67b0ca56f5832a3ac4" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-4c67b0ca56f5832a3ac4">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"],["England","Austria","France","Germany","Italy","Canada","Slovenia","Morocco","Turkey","Portugal","Albania","US-France","Australia","US","Serbia"],[92.89,89.28,88.93,88.63,88.41,88.24,88.23,88.17,88.1,88.06,88,88,87.89,87.82,87.71],[47.5,31.19,45.62,39.01,37.55,34.63,28.06,18.83,25.8,26.33,20,50,31.26,33.65,24.29],[9,3057,21098,2452,23478,196,94,12,52,5322,2,1,4957,62397,14]],"container":"<table class=\"cell-border stripe\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>country<\/th>\n      <th>Points<\/th>\n      <th>Price<\/th>\n      <th>n<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"digits":2,"dom":"tip","columnDefs":[{"className":"dt-right","targets":[2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
]

---

Before you start any sort of analysis, always a good idea to take a look at the data.

.pull-left[

```r
wine_subset &lt;- wine %&gt;% filter(country %in% c('France', 'Turkey'))

ggplot(wine_subset,
       aes(x = country, y = points, fill = country)) + 
  scale_fill_viridis_d() + guides(fill = "none") +
  geom_boxplot(alpha = 0.5) 

ggplot(wine_subset,
       aes(x = points, fill = country)) + 
  scale_fill_viridis_d() + guides(fill = "none") +
  geom_histogram(aes(y = after_stat(density)),
                 position = "identity", alpha = 0.5,
                 bins = 20) 
```

From these plots:

* though means are close, when compared to spreads, could be a difference

* variance do NOT look equal

]

.pull-right[
&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-4-1.png" width="400px" height="250px" style="display: block; margin: auto;" /&gt;&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-4-2.png" width="400px" height="250px" style="display: block; margin: auto;" /&gt;
]

---

Want to test `\(H_0: \mu_\text{France} = \mu_\text{Turkey}\)` against the alternative `\(H_A: \mu_\text{France} \neq \mu_\text{Turkey}\)`. We'll use `\(\alpha = 0.05\)`. 

We want to do this using a two sample `\(t\)`-test. First, need to check assumptions:

--

* independent groups

--

* independent observations

--

* are averages normally distributed?
--
 both `\(n\)`'s are greater than `\(30\)`, so CLT.

--

Next, need to know if we can assume equal variances. So, find standard deviations for the two groups.

.pull-left[

```r
wine_subset %&gt;% 
  group_by(country) %&gt;% 
  summarize(means = mean(points),
            s = sd(points),
            n = n())
```
]

.pull-right[

```
## # A tibble: 2 x 4
##   country means     s     n
## * &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 France   88.9  3.20 21098
## 2 Turkey   88.1  1.58    52
```
]

Since `\(s_\text{France}/s_\text{Turkey} &gt; 2\)`, variances cannot be assumed equal. 

Good time to pause and double check with plots: this matches what we saw. Nice!

---

So, we cannot assume equal variances. Hence, we calculate `\(\widehat{\text{SD}}(V) = \sqrt{\frac{s_\text{France}^2}{n_\text{France}} + \frac{s_\text{Turkey}^2}{n_\text{Turkey}}}\)`, and we then know that **IF** the null hypothesis is true, then `\(T \sim t_\nu\)` where 

`$$\nu = \frac{\left(\frac{s_\text{France}^2}{n_\text{France}} + \frac{s_\text{Turkey}^2}{n_\text{Turkey}} \right)^2}{\frac{(s_\text{France}^2/n_\text{France})^2}{n_\text{France}-1} + \frac{(s_\text{Turkey}^2/n_\text{Turkey})^2}{n_\text{Turkey}-1}}$$`

So, let us calculate the two.

.pull-left[

```r
wine_subset %&gt;% 
  group_by(country) %&gt;% 
  summarize(s2 = var(points),
            n = n()) %&gt;% print %&gt;% 
  summarize(sd_V = sqrt(sum( s2 / n )),
            df = sum( s2 / n )^2 / (sum( (s2 / n)^2 / (n - 1) )))
```
]

.pull-right[

```
## # A tibble: 2 x 3
##   country    s2     n
## * &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;
## 1 France  10.2  21098
## 2 Turkey   2.48    52
```

```
## # A tibble: 1 x 2
##    sd_V    df
##   &lt;dbl&gt; &lt;dbl&gt;
## 1 0.220  52.0
```
]

---

So, we can find `\(T_\text{obs}\)` (I use more digits than previously presented)


```r
T_obs &lt;- (88.92587 - 88.09612 - 0)/0.2195276; T_obs
```

```
## [1] 3.779707
```

--

and compare it to the `\(t\)`-distribution with `\(52\)` degrees of freedom:

&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-8-1.png" height="375px" style="display: block; margin: auto;" /&gt;

---

Conclusion using quantiles:

1. find values such that there's `\(\alpha/2 = 0.025\)` to the left and right, respectively: 
    
    ```r
    T_52 &lt;- StudentsT(df = 52)
    quantile(T_52, c(0.025, 0.975))
    ```
    
    ```
    ## [1] -2.006647  2.006647
    ```
  these are our cut-offs for when the observed value of the test statistic `\(T_\text{obs}\)` is far from `\(0\)`
--

2. check if our value is outside this interval
    * since we found that `\(T_\text{obs} = 3.779707\)`, it is outside the cut-offs
    
--

3. since `\(T_\text{obs}\)` is outside the cut-offs, it *is* far from `\(0\)`

--

4. since `\(T_\text{obs}\)` is far from `\(0\)`, `\(\bar{X}_\text{France} - \bar{X}_\text{Turkey}\)` *is* far from `\(0\)`, i.e. `\(\bar{X}_\text{France}\)` *is* far from `\(\bar{X}_\text{Turkey}\)`

--

5. since `\(\bar{X}_\text{France}\)` is far from `\(\bar{X}_\text{Turkey}\)`, we no longer believe that `\(\mu_\text{France} = \mu_\text{Turkey}\)`.


---

&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-10-1.png" width="900px" height="500px" style="display: block; margin: auto;" /&gt;


---

Conclusion using the p-value:

1. find the probability of being further from zero:
    
    ```r
    2*(1 - cdf(T_52, T_obs))
    ```
    
    ```
    ## [1] 0.0004060735
    ```
--

2. since the probability of being further away from zero is less than `\(\alpha = 0.05\)`, it *is* small

--

3. since the probability of being further away from zero is small, `\(T_\text{obs}\)` *is* far from zero.

--

4. since `\(T_\text{obs}\)` is far from `\(0\)`, `\(\bar{X}_\text{France} - \bar{X}_\text{Turkey}\)` *is* far from `\(0\)`, i.e. `\(\bar{X}_\text{France}\)` *is* far from `\(\bar{X}_\text{Turkey}\)`

5. since `\(\bar{X}_\text{France}\)` is far from `\(\bar{X}_\text{Turkey}\)`, we no longer believe that `\(\mu_\text{France} = \mu_\text{Turkey}\)`.


---

&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-12-1.png" width="900px" height="500px" style="display: block; margin: auto;" /&gt;

---

We can also calculate a `\(95\%\)` confidence interval:


```r
wine_subset %&gt;% 
  group_by(country) %&gt;% 
  summarize(means = mean(points),
            s = sd(points),
            n = n())
```

```
## # A tibble: 2 x 4
##   country means     s     n
## * &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 France   88.9  3.20 21098
## 2 Turkey   88.1  1.58    52
```

`$$\begin{aligned}
\hat{V} \pm t_{52, 0.05/2} \widehat{\text{SD}}(V) &amp;= (88.92587 - 88.09615) \pm 2.006647 \sqrt{\frac{3.199695^2}{21098} + \frac{1.575046^2}{52}} \\
&amp;= 0.82972 \pm 0.4405144
\end{aligned}$$`

So, we are `\(95\%\)` confident that the true difference in mean points for wines from France vs wines from Turkey is in the interval [0.39, 1.27]. 

---

Using `t.test` to double check our results:



```r
t.test(data = wine_subset, 
       points ~ country, var.equal = FALSE)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  points by country
## t = 3.7796, df = 52.043, p-value = 0.000406
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.3892102 1.2702216
## sample estimates:
## mean in group France mean in group Turkey 
##             88.92587             88.09615
```


---

**Revisit: Corona Virus data**

Previously, we considered the death rates in Italy and China. Back then, we didn't have the tools to actually test the hypothesis that they are the same.

Let's consider the hypothesis `\(H_0: \pi_\text{Italy} = \pi_\text{China}\)`, and test it against `\(H_A:\pi_\text{Italy} \neq \pi_\text{China}\)` using `\(\alpha = 0.01\)`.

Most recent data:




```r
corona_subset
```

```
## # A tibble: 2 x 5
##   Country deaths confirmed recovered  p_hat
## * &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 China     3240     79243     71060 0.0409
## 2 Italy     6820     69176      8326 0.0986
```

---

We will reject null if `\(P_\text{Italy}\)` is far from `\(P_\text{China}\)`. So, we will consider `\(P_\text{Italy} - P_\text{China}\)`. 

To be able to say if they are "far from each other", we want to find the probability that they are further from each other. So, we need the distribution of some quantity we know. 

Remember, we also do this assuming `\(H_0\)` is true. 

Remember, `\(P_\text{Italy}\)` and `\(P_\text{China}\)` are proportions, so approximately normally distributed if the `\(n\)`'s are large enough. 

So, if the `\(n\)`'s are large enough, the difference will be normally distributed with mean `\(0\)`. But what is the variance?

--

If `\(H_0\)` is true, then `\(\pi_\text{Italy} = \pi_\text{China}\)`. Let's call this common proportion `\(\pi_0\)`. So,

`$$\begin{aligned}
\text{Var}(P_\text{Italy}) &amp;= \frac{\pi_\text{Italy} (1 - \pi_\text{Italy})}{n_\text{Italy}} = \frac{\pi_0 (1 - \pi_0)}{n_\text{Italy}} \\
\text{Var}(P_\text{China}) &amp;= \frac{\pi_\text{China} (1 - \pi_\text{China})}{n_\text{China}} = \frac{\pi_0 (1 - \pi_0)}{n_\text{China}}.
\end{aligned}$$`

---

Therefore, assuming the two groups are independent,

`$$\text{Var}(P_\text{Italy} - P_\text{China}) = \text{Var}(P_\text{Italy}) + \text{Var}(P_\text{China}) = \pi_0 (1-\pi_0)\left(\frac{1}{n_\text{Italy}} + \frac{1}{n_\text{China}}\right).$$`

So, if the null hypothesis is true,

`$$P_\text{Italy} - P_\text{China} \sim N\left(0, \pi_0(1-\pi_0)\left(\frac{1}{n_\text{Italy}} + \frac{1}{n_\text{China}}\right)\right),$$`

or equivalently,

`$$Z = \frac{P_\text{Italy} - P_\text{China}}{\sqrt{\pi_0(1-\pi_0)\left(\frac{1}{n_\text{Italy}} + \frac{1}{n_\text{China}}\right)}} \sim N\left(0, 1\right).$$`

Notice how this is of the form `\(\frac{V - v_0}{\widehat{\text{SD}}(V)}\)` **IF** the null hypothesis is true.

---

There are a few questions we need to answer:

1. how big do `\(n_\text{Italy}, n_\text{China}\)` have to be?

2. how should we estimate `\(\pi_0\)`?

--

To answer the first question: we need the sample sizes big enough that `\(P_\text{Italy}\)` and `\(P_\text{China}\)` are approximately normally distributed when the null hypothesis is true. Previously, we said that if `\(\pi_\text{Italy}n_\text{Italy} &gt; 5\)` and `\((1 - \pi_\text{Italy})n_\text{Italy} &gt; 5\)`, then all is well. (Same for `\(n_\text{China}\)`.)

Will use same rule of thumb here. But remember, when `\(H_0\)` is true, `\(\pi_\text{Italy} = \pi_\text{China} = \pi_0\)`. So, we need 

`$$\pi_0 n_\text{Italy} &gt; 5 \quad \text{ and } (1 - \pi_0)n_\text{Italy} &gt; 5$$`

and

`$$\pi_0n_\text{China} &gt; 5 \quad \text{ and } (1 - \pi_0)n_\text{China} &gt; 5$$`

---

What should we use to estimate `\(\pi_0\)`? Well, if the null hypothesis is true, the two groups are basically the same (in terms of the true proportion, at least). So, to estimate `\(\pi_0\)`, we will treat the two groups as one big group:

`$$P_0 = \frac{1}{n}\sum_{i=1}^{n} X_i = \frac{n_\text{Italy} P_\text{Italy} + n_\text{China} P_\text{China}}{n_\text{Italy} + n_\text{China}}$$`

---

So, first we will find our observed value of `\(P_0\)`:


```r
corona_subset %&gt;% summarize(p_0 = sum(p_hat*confirmed)/(sum(confirmed)))
```

```
## # A tibble: 1 x 1
##      p_0
##    &lt;dbl&gt;
## 1 0.0678
```

Next, we check if the two sample sizes are big enough:


```r
corona_subset %&gt;% 
  mutate(p_0 = sum(p_hat*confirmed)/(sum(confirmed)),
         check_n1 = confirmed*p_0,
         check_n2 = confirmed*(1-p_0))
```

```
## # A tibble: 2 x 8
##   Country deaths confirmed recovered  p_hat    p_0 check_n1 check_n2
## * &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 China     3240     79243     71060 0.0409 0.0678    5371.   73872.
## 2 Italy     6820     69176      8326 0.0986 0.0678    4689.   64487.
```



---

Finally, calculate the observed value of our test statistic:

`$$Z_\text{obs} = \frac{0.09858911 - 0.04088689}{\sqrt{0.06778108\cdot (1-0.06778108)\left(\frac{1}{69176} + \frac{1}{79243}\right)}} = 44.1156569$$`

We compare this to the standard normal.

---

Conclusion using quantiles: is our observed value "far from 0"? Find cut-offs such that only `\(\alpha/2 = 0.005\)` is further from 0 on each side:


```r
quantile(Normal(), c(0.005, 0.995))
```

```
## [1] -2.575829  2.575829
```

Since we observed 44.1156569, we observe something far from `\(0\)`. 

So, `\(P_\text{Italy}\)` is far from `\(P_\text{China}\)`. 

So, we reject the idea that `\(\pi_\text{Italy} = \pi_\text{China}\)`. 

&lt;img src="lec16_slides_files/figure-html/unnamed-chunk-21-1.png" width="600px" height="200px" style="display: block; margin: auto;" /&gt;


---

As always, we are probably more interested in a confidence interval than a hypothesis test. Fortunately, we can "easily" create that. However, we need to take a step back. 

We would like to simply take our test statistic, `\(\frac{P_\text{Italy} - P_\text{China}}{\sqrt{\pi_0(1-\pi_0)\left(\frac{1}{n_\text{Italy}} + \frac{1}{n_\text{China}}\right)}}\)`, and rearrange it. However, this only follows a normal distribution **IF** the null hypothesis is true. 

Therefore, we need to be a bit more general, and instead use `\(\frac{P_\text{Italy} - P_\text{China}}{\sqrt{\left(\frac{P_\text{Italy}(1-P_\text{Italy})}{n_\text{Italy}} + \frac{P_\text{China}(1-P_\text{China})}{n_\text{China}}\right)}}\)`. We can then construct a `\((1-\alpha)\%\)` CI as


`$$P_\text{Italy} - P_\text{China} \pm z_{\alpha/2} \sqrt{\left(\frac{P_\text{Italy}(1-P_\text{Italy})}{n_\text{Italy}} + \frac{P_\text{China}(1-P_\text{China})}{n_\text{China}}\right)}.$$`

Notice, this is of the form `\(V \pm z_{\alpha/2} \widehat{\text{SD}}(V)\)`. 

We use `\(Z\)` instead of `\(T\)` here, since `\(V \sim N\)`, and we only do not have to estimate the standard deviation separate from estimating the means. 

---

Calculations:


```r
corona_subset %&gt;% print %&gt;% 
  summarize(LL = p_hat[2] - p_hat[1] - 2.576*sqrt(sum(p_hat*(1-p_hat)/confirmed)),
            UL = p_hat[2] - p_hat[1] + 2.576*sqrt(sum(p_hat*(1-p_hat)/confirmed)))
```

```
## # A tibble: 2 x 5
##   Country deaths confirmed recovered  p_hat
## * &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 China     3240     79243     71060 0.0409
## 2 Italy     6820     69176      8326 0.0986
```

```
## # A tibble: 1 x 2
##       LL     UL
##    &lt;dbl&gt;  &lt;dbl&gt;
## 1 0.0543 0.0611
```

---

We can do this in `R` using the function `prop.test`:


```r
prop.test(x = c(6820, 3240), n = c(69176, 79243), correct = FALSE, conf.level = 0.99)
```

```
## 
## 	2-sample test for equality of proportions without continuity
## 	correction
## 
## data:  c(6820, 3240) out of c(69176, 79243)
## X-squared = 1946.2, df = 1, p-value &lt; 2.2e-16
## alternative hypothesis: two.sided
## 99 percent confidence interval:
##  0.05426606 0.06113837
## sample estimates:
##     prop 1     prop 2 
## 0.09858911 0.04088689
```

Important things to know about `prop.test`:

.pull-left[
* it uses `\(Z^2\)` as the test statistic instead of `\(Z\)`
    - notice how `\(\sqrt{1946.2} = 44.1157568 \approx 44.1156569\)`
]
.pull-right[
* you need to specify `correct = FALSE`
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:10",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
