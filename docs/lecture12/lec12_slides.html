<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 12: One Sample Hypothesis Tests</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ralph Trane" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="libs/jquery-1.12.4/jquery.min.js"></script>
    <link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding-0.9/datatables.js"></script>
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core-1.10.19/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core-1.10.19/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../css/uwmadison.css" type="text/css" />
    <link rel="stylesheet" href="../css/extra-classes.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, .title-slide, title-slide

# Lecture 12: One Sample Hypothesis Tests
## STAT 324
### Ralph Trane
### University of Wisconsinâ€“Madison<br><br>
### Spring 2020

---

layout: true

# One Sample Hypothesis Test: Example I

---




Recall: car manufacturer uses automatic device to apply paint to engine blocks. It is important that the paint is a certain thickness, so it can resist the heat produced by the engine when running.

The manufacturer knows that as long as the thickness is greater than 1.5mm, all is good. They are willing to accept some loss, so they really just want to make sure that, on average, the thickness is greater than 1.5mm. 

The manufacturer decides to hire you as a consultant. They want your help to answer the question: "is the mean thickness greater than 1.5mm?" 

--

As a statistical wizard, you know to test such a hypothesis, the first thing to do is to setup a null and alternative hypothesis. Recall: the null is always the simplest/least interesting hypothesis. So, if we let `\(\mu\)` denote the mean paint thickness, we would like to test the hypothesis

$$
  H_0: \mu = 1.5 \qquad \text{against} \qquad H_A: \mu &lt; 1.5.
$$

--

Next, we have to decide on a reasonable significance level, `\(\alpha\)`. Remember, `\(\alpha\)` is the probability of making a Type I error. I.e. `\(\alpha = P(\text{reject } H_0\ |\ H_0 \text{ true})\)`. 

We pick `\(\alpha = 0.05\)`. I.e. we decide that it is acceptable to us to rejecting the null, even if it is true, `\(5\%\)` of the time. 

---

We collect a sample of 16 engines, and measure the paint thickness. Our first step is to visualize the data:


```r
library(tidyverse); library(distributions3); theme_set(theme_bw())
paint_thickness &lt;- tibble(
  thickness = c(1.29, 1.12, 0.88, 1.65, 1.48, 1.59, 1.04, 0.83, 
                1.76, 1.31, 0.88, 1.71, 1.83, 1.09, 1.62, 1.49))

ggplot(data = paint_thickness,
       aes(y = thickness)) + 
  geom_boxplot(width = 0.5) + 
  geom_jitter(aes(x = 0),
              width = 0.2) + 
  geom_hline(aes(yintercept = 1.5,
                 color = "Null hypothesis")) +
  geom_hline(aes(yintercept = mean(thickness),
                 color = "Sample average")) +
  xlim(c(-1,1))
```


---

We collect a sample of 16 engines, and measure the paint thickness. Our first step is to visualize the data:

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-2-1.png" height="375px" style="display: block; margin: auto;" /&gt;


The question then is: are the two lines so far apart that we do no longer think that `\(\mu = 1.5\)` is reasonable? Does the data seem to suggest we should change our beliefs? 

---

The next step is to find a good measure of what we are looking for: difference between the observed ( `\(\bar{x}\)` ) and the null hypothesis ( `\(\mu = 1.5\)` ) relative to the variation in the data ( `\(\sigma\)` ) and sample size ( `\(n\)` ). 


Maybe `\(T = \frac{\bar{X} - 1.5}{\sigma/\sqrt{16}}\)` will work? 

--

We do not know `\(\sigma\)`, so would need to consider `\(T = \frac{\bar{X} - 1.5}{s/\sqrt{16}}\)`. To find out if this would work, we want to ask: can we find `\(P(T \text{ more extreme than } T_{\text{obs}} | H_0 \text{ is true})\)`? To find this, we need to find the distribution of `\(T\)` **ASSUMING** the null hypothesis is true. I.e. we will pretend `\(\mu = 1.5\)` is the true mean paint thickness. If this is the case, 

$$
  T = \frac{\bar{X} - 1.5}{s/\sqrt{16}} = \frac{\bar{X} - E(\bar{X})}{\widehat{\text{SD}}(\bar{X})}.
$$

We know that this follows a `\(t\)`-distribution with `\(n-1\)` degrees of freedom, **IF** `\(\bar{X}\)` is normally distributed. 

---

Since `\(n &lt; 30\)`, need the data to be normal for `\(\bar{X} \sim N\)`. QQ-plot:


```r
ggplot(data = paint_thickness, aes(sample = thickness)) +
  geom_qq() + 
  geom_qq_line() +
  labs(title = "QQ-plot for paint thickness data")
```

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-3-1.png" height="350px" style="display: block; margin: auto;" /&gt;

Seems like it is not crazy to think the data is normally distributed.

---

So, we know that `\(T = \frac{\bar{X} - 1.5}{s/\sqrt{16}} \sim t_{n-1}\)` **IF** the null hypothesis `\(\mu = 1.5\)` is true. 

Next, calculate the observed value of `\(T\)`:


```r
paint_thickness %&gt;% 
  summarize(xbar = mean(thickness),
            s = sd(thickness),
            T_obs = (xbar - 1.5)/(s/sqrt(16)))
```

```
## # A tibble: 1 x 3
##    xbar     s T_obs
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1.35 0.339 -1.79
```

Recall, `\(T_{\text{obs}} &lt;&lt; 0\)` if `\(\bar{x} &lt;&lt; 1.5\)`, and close to `\(0\)` if `\(\bar{x}\)` is close to `\(1.5\)`. So, want to ask: is `\(T_{\text{obs}}\)` so small (i.e. very negative) that we do not think the true mean is `\(1.5\)`, but instead think that `\(\mu &lt; 1.5\)`? 

---

**IF** the null is true, `\(T_{\text{obs}} \sim t_{15}\)`. So, **IF** the null is true, and `\(T_{\text{obs}}\)` is very small, then `\(P(T_{15} &lt; T_{\text{obs}})\)` must be very small. Remember, here "small" means "smaller than `\(\alpha\)`". We picked `\(0.05\)`.

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-5-1.png" height="300px" style="display: block; margin: auto;" /&gt;

---

We find `\(P(T_{15} &lt; T_{obs}) = P(T_{15} &lt; -1.795)\)`:


```r
cdf(T_15, -1.795)
```

```
## [1] 0.04641306
```

Since this is greater than `\(\alpha\)`, we conclude that `\(T_{obs} = -1.795\)` is NOT crazy small.

I.e. `\(\bar{x}\)` is NOT much smaller than `\(1.5\)`.

So, we do NOT reject the idea the the true mean is in fact `\(1.5\)`. 

---
 
Summary:

* Want to test `\(H_0: \mu = 1.5\)` against `\(H_A: \mu &lt; 1.5\)` using `\(\alpha = 0.05\)`.

--

* Find that we can assume the data are normal from QQ-plot, so `\(\bar{X}\)` is normal.

--

* **IF** `\(\mu = 1.5\)` is true, then `\(T = \frac{\bar{X} - 1.5}{s/\sqrt{16}} = \frac{\bar{X} - E(\bar{X})}{\widehat{\text{SD}}(\bar{X})} \sim t_{15}\)`.

--

* **IF** `\(\mu = 1.5\)`, then the probability of something "more extreme" is `\(P(T_{15} &lt; T_{\text{obs}}) \approx 0.33\)`.

--

* **IF** `\(\mu = 1.5\)`, then something "more extreme" isn't super unlikely, so our observation is not that unlikely

--

* Since our observation isn't unlikely **IF** `\(\mu = 1.5\)`, we do not reject `\(H_0: \mu = 1.5\)`

---

On second thought, the manufacturer is actually also interested in the mean paint thickness not beeing TOO large. Thicker layer of paint, more materials used, increased cost. 

--

So, they would like to test the hypothesis

$$
  H_0: \mu = 1.5 \quad \text{against} \quad H_A: \mu \neq 1.5.
$$

Once again, we pick `\(\alpha = 0.05\)`. 

Most of the procedure is the same: 

* we still ask, if `\(\bar{X}\)` is close enough to `\(1.5\)` that we do still believe in the null hypothesis. 

* we still answer in terms of the difference compared to the variation: 

`$$T = \frac{\bar{X} - 1.5}{\sigma/\sqrt{16}}$$`

* `\(\bar{X}\)` is still normally distributed (the QQ-plot is the same, so data still normal...), so `\(T \sim t_{15}\)`

The difference is in what it means to be "more extreme".

---

First, remember that `\(T_{\text{obs}} = -1.795\)`. So, what does it mean to be "more extreme"?
--
 Since the alternative is `\(H_A: \mu \neq 1.5\)`, "more extreme" goes in both directions: "more extreme" is "further from 0", i.e. outside the dotted lines below:

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-7-1.png" height="375px" style="display: block; margin: auto;" /&gt;

---

So, the p-value is 

`$$\begin{aligned}
P(\text{more extreme}\ |\ H_0 \text{ true}) &amp;= P(T &lt; -1.795 \text{ OR } T &gt; 1.795) \\
&amp;= P(T &lt; -1.795) + P(T &gt; 1.795) \\
&amp;= 2\cdot P(T &lt; -1.795)
\end{aligned}$$`
 
Found in `R`:

.pull-left[

```r
p_value &lt;- 2*cdf(T_15, -1.795)
p_value
```
]

.pull-right[

```
## [1] 0.09282611
```
]

--

Since `\(p &gt; 0.05\)`, we do NOT reject the null hypothesis:

* **IF** the null is true, the probability of seeing something "futher away" from the null is large

* **IF** the null is true, our observed data are NOT unlikely

* since our observed is NOT unlikely **IF** the null is true, our observed data provides little evidence against the null

* we do NOT reject the null.


---
layout: true

# One Sample Hypothesis Test: Example II

---
 
Recall: Framingham Heart Study. Interested in the true population mean total cholesterol. In particular, is the population mean total cholesterol 230 mg/dL?

--

To perform a hypothesis test, we setup our null- and alternative hypotheses: we will test

$$
  H_0: \mu = 230 \quad \text{against} \quad H_A: \mu \neq 230.
$$

This time, we pick `\(\alpha = 0.01\)`, i.e. we will only accept making a Type I error `\(1\%\)` of the time. 

---

We'll take a look at the data:


```r
fram &lt;- read_csv(here::here("csv_data/framingham.csv")) %&gt;% 
  filter(!is.na(totChol))

ggplot(fram,
       aes(y = totChol)) +
  geom_boxplot(width = 0.5) + 
  geom_jitter(aes(x = 0), 
              height = 0, width = 0.2, alpha = 0.05) +
  geom_hline(aes(yintercept = 230, 
                 color = "Null hypothesis")) +
  geom_hline(aes(yintercept = mean(totChol), 
                 color = "Sample average"))
```

---

We'll take a look at the data:

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-9-1.png" height="375px" style="display: block; margin: auto;" /&gt;


---

Question: is our sample average far from `\(230\)` mg/dL?

Once again, we will assess this using `\(T = \frac{\bar{X} - 230}{\widehat{\text{SD}}(\bar{X})} = \frac{\bar{X} - 230}{s/\sqrt{n}}\)`. 

**IF** the null hypothesis is true, then `\(E(\bar{X}) = 230\)`. 

Therefore, **IF** the null hypothesis is true, 

$$
  T = \frac{\bar{X} - 230}{s/\sqrt{n}} = \frac{\bar{X} - E(\bar{X})}{\widehat{\text{SD}}(\bar{X})} \sim t_{n-1}.
$$

Let's find `\(T_{\text{obs}}\)`:


```r
fram %&gt;% 
  summarize(xbar = mean(totChol),
            SD = sd(totChol),
            n = n(),
            T_obs = (xbar - 230)/(SD/sqrt(n)))
```

```
## # A tibble: 1 x 4
##    xbar    SD     n T_obs
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;
## 1  237.  44.6  4190  9.73
```

---

So, is `\(9.725\)` far from `\(0\)`? 

**IF** the null hypothesis is true, what is the probability of being further away? The area outside of:

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-11-1.png" height="375px" style="display: block; margin: auto;" /&gt;

---

Find the p-value: 

$$
`\begin{aligned}
P(\text{more extreme}\ |\ H_0 \text{ true}) &amp;= P(T_{4189} &gt; 9.725) + P(T_{4189} &lt; -9.725) \\
&amp;= 2*P(T_{4189} &lt; -9.725)
\end{aligned}`
$$

Very small:


```r
T_4189 &lt;- StudentsT(df = 4189)

2*cdf(T_4189, -9.725)
```

```
## [1] 4.036102e-22
```

Since `\(p &lt; \alpha\)`, the probability of something "more extreme" is small, so our data is very extreme. 

Our data are far from the null hypothesis, so why would we still think the null hypothesis is true? We reject the null. 

---

This is not very constructive: sure, the true mean is not `\(230\)` mg/dL... but what is it then? 

Let's try to characterize all the values of `\(\mu_0\)` that we would NOT reject. I.e. which `\(H_0: \mu = \mu_0\)` would not be rejected? 

--

If `\(\bar{X}\)` is close enough to `\(\mu_0\)`, then we wouldn't not reject the null.

"Close enough" means that the probability of being "more extreme" is greater than `\(\alpha = 0.05\)`. In other words, `\(T = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}\)` has to be so close to zero that 

$$
`\begin{aligned}
P(\text{more extreme}\ |\ H_0) &amp;= P(T_{n-1} &gt; |T_{\text{obs}}|) + P(T_{n-1} &lt; -|T_{\text{obs}}|) \\
                               &amp;= 2\cdot P(T_{n-1} &gt; |T_{\text{obs}}|) &lt; 0.05. 
\end{aligned}`
$$

I.e. `\(P(T_{n-1} &gt; |T_{\text{obs}}|) = 0.025\)`. 

---

That means that if `\(T_{\text{obs}} &gt; t_{n-1, 0.025}\)` or `\(T_{\text{obs}} &lt; -t_{n-1, 0.025}\)`, then we would reject. Or, in other words, if `\(-t_{n-1, 0.025} &lt; T_{\text{obs}} &lt; t_{n-1, 0.025}\)`, then we would not reject. So, we do NOT reject when

`$$-t_{n-1, 0.025} &lt; \frac{\bar{X} - \mu_0}{s/\sqrt{n}} &lt; t_{n-1, 0.025}$$`

--

or equivalently, when

`$$\bar{X}-t_{n-1, 0.025}\frac{s}{\sqrt{n}} &lt; \mu_0 &lt; \bar{X} + t_{n-1,0.025}\frac{s}{\sqrt{n}}$$`

--

This is **EXACTLY** the `\(95\%\)` confidence interval we would calculate for `\(\mu\)`!!

&lt;img src="/Users/ralphtrane/Documents/UW-Madison/STAT324/Spring2020/Lectures/lecture12/images/wow.gif" width="480px" height="252px" style="display: block; margin: auto;" /&gt;

---

So, 

$$
`\begin{equation}
  \text{reject } H_0: \mu = \mu_0 \text{ against } H_A: \mu \neq \mu_0 \text{ at significance level } \alpha \\
  \iff \\
  \mu_0 \text{ is NOT in the } (1-\alpha)\cdot 100 \% \text{ CI for } \mu
\end{equation}`
$$

This supports our interpretation of CIs as a "collection of plausible values", i.e. values we wouldn't reject based on the data we have.

---

For the total cholesterol, let's find a `\(95\%\)` confidence interval:


```r
T_4189 &lt;- StudentsT(df = 4189)

fram %&gt;% 
  summarize(xbar = mean(totChol),
            SD = sd(totChol),
            n = n(),
            LL = xbar - quantile(T_4189, 0.975)*SD/sqrt(n),
            UL = xbar + quantile(T_4189, 0.975)*SD/sqrt(n))
```

```
## # A tibble: 1 x 5
##    xbar    SD     n    LL    UL
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  237.  44.6  4190  235.  238.
```

As expected, the value we just rejected (230 mg/dL) is NOT in the `\(95\%\)` confidence interval. 

---
layout: true

# One Sample Hypothesis Test: Example I (revisited)

---

Let's revisit the paint thickness data and calculate a `\(95\%\)` confidence interval:


```r
paint_thickness_CI &lt;- paint_thickness %&gt;% 
  summarize(xbar = mean(thickness),
            SD = sd(thickness),
            n = n(),
            LL = xbar - quantile(StudentsT(df = n - 1), 0.975)*SD/sqrt(n),
            UL = xbar + quantile(StudentsT(df = n - 1), 0.975)*SD/sqrt(n))

paint_thickness_CI
```

```
## # A tibble: 1 x 5
##    xbar    SD     n    LL    UL
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1.35 0.339    16  1.17  1.53
```

We see that the value we did not reject, i.e. 1.5, is indeed in the `\(95\%\)` confidence interval, as we would expect. 

---


```r
ggplot(data = paint_thickness,
       aes(x = thickness, y = 1)) + 
  geom_jitter(width = 0, height = 0.1) + 
  ## Below we add some things to the plot using a different data set,
  ## namely paint_thickness_CI, which has the things we just calculated
  geom_errorbarh(data = paint_thickness_CI,
                 aes(xmin = LL, xmax = UL, x = NULL),
                 height = 0.1, color = "red") +
  geom_point(data = paint_thickness_CI,
             aes(x = xbar), color = "red") +
  scale_y_continuous(limits = c(0.8, 1.2), breaks = NULL, minor_breaks = NULL) ## y-axis stuff
```

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-16-1.png" height="200px" style="display: block; margin: auto;" /&gt;

---
layout: true

# One Sample Hypothesis Test: Example III

---

.small[

```r
library(lubridate)
corona_virus &lt;- read_csv(here::here("csv_data/corona_virus.csv")) %&gt;% 
  mutate(date = mdy(date))

corona_virus %&gt;% group_by(`Country/Region`, `Province/State`) %&gt;% tail(20) %&gt;% DT::datatable(options = list(pageLength = 5, scrollX = T))
```

<div id="htmlwidget-35801c5e22b42c43ddca" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-35801c5e22b42c43ddca">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20"],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],["Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis","Saint Kitts and Nevis"],[17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822,17.357822],[-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998,-62.782998],["2020-03-06","2020-03-07","2020-03-08","2020-03-09","2020-03-10","2020-03-11","2020-03-12","2020-03-13","2020-03-14","2020-03-15","2020-03-16","2020-03-17","2020-03-18","2020-03-19","2020-03-20","2020-03-21","2020-03-22","2020-03-23","2020-03-24","2020-03-25"],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Province/State<\/th>\n      <th>Country/Region<\/th>\n      <th>Lat<\/th>\n      <th>Long<\/th>\n      <th>date<\/th>\n      <th>confirmed<\/th>\n      <th>deaths<\/th>\n      <th>recovered<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[3,4,6,7,8]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
]

---

Using this data, we can estimate the risk of dying:


```r
corona_virus %&gt;% 
  filter(date == "2020-03-09") %&gt;% 
  summarize_at(.vars = vars(confirmed, deaths, recovered),
               .funs = sum, na.rm = T) %&gt;% 
  mutate(P = deaths/confirmed)
```

```
## # A tibble: 1 x 4
##   confirmed deaths recovered      P
##       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1    113590   3988     60266 0.0351
```


---

Obviously many problems with this:

* data highly dependent

* would definitely need a time component

* ...

For now, let's ignore this.

---

Researchers might be interested in knowing if the risk of dying from the corona virus is dependent on region. They want to test if the risk in Italy is the same as the risk in Mainland China. 

**NOTE**: the following is not how you would actually do this, but is the best we can do with our current tools. We will revisit this later.

First, they estimate the risk in Mainland China and Italy:


```r
corona_virus %&gt;% 
  filter(`Country/Region` %in% c("Italy", "China"),
         date == "2020-03-04") %&gt;% 
  group_by(`Country/Region`) %&gt;% 
  summarize_at(vars(confirmed, deaths, recovered),
               sum, na.rm = T) %&gt;% 
  mutate(P = deaths/confirmed)
```

```
## # A tibble: 2 x 5
##   `Country/Region` confirmed deaths recovered      P
## * &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 China                80386   2983     47927 0.0371
## 2 Italy                 3089    107       276 0.0346
```



---

If we take the estimated risk in Mainland China as "the true risk" in Mainland China, we can test `\(H_0: \pi_{\text{Italy}} = 0.0371\)` vs. `\(H_A: \pi_{\text{Italy}} \neq 0.0371\)`.

We will use `\(\alpha = 0.1\)`, and estimate `\(\pi_{\text{Italy}}\)` using `\(P = \frac{\#\text{deaths}}{\#\text{confirmed}}\)`
--
 `\(= \frac{1}{n}\sum_{i=1}^n X_i\)`, where `\(X_i \sim \text{Bernoulli}(\pi_{\text{Italy}})\)`.

--

As always, we want to see if our estimate `\(P\)` is far from our null `\(0.0371\)`, and we still want to compare it to a measure of variation of our estimator `\(P\)`. So think about using 

$$
  \frac{P - 0.0371}{\text{SD}(P)}
$$

--

**IF** the null hypothesis is true, then `\(0.0371 = E(P)\)`. 

Remember, `\(\text{SD} =\sqrt{\pi_{\text{Italy}}(1-\pi_{\text{Italy}})/n}\)`. **IF** the null hypothesis is true, then `\(\pi_{\text{Italy}} = 0.0371\)`, so `\(\text{SD}(P) = \sqrt{0.0371\cdot (1-0.0371)/n}\)`. 

So **IF** the null hypothesis is true, then 

`$$\frac{P - 0.0371}{\sqrt{\frac{0.0371(1-0.0371)}{3089}}} = \frac{P - E(P)}{\text{SD}(P)}.$$`

---

Now, remember that `\(P \sim N\)`, if `\(n\cdot \pi_{\text{Italy}} &gt; 5\)` and `\(n(1-\pi_{\text{Italy}}) &gt; 5\)`. 

--

**IF** the null is true, then `\(P \sim N\)` if `\(n \cdot 0.0371 &gt; 5\)` and `\(n \cdot (1-0.0371) &gt; 5\)`. 

--

`\(n\)` is the number of confirmed in Italy, so `\(n = 3089\)`. Since `\(3089 \cdot 0.0371 = 114.6019\)` and `\(3089 \cdot (1-0.0371) = 2974.3981\)`, the CLT tells us that `\(P \sim N\)`, **IF** `\(H_0\)` is true.

--

So, **IF** `\(H_0\)` is true, then 

$$
Z = \frac{P - E(X)}{\text{SD}(P)} = \frac{P - 0.0371}{\sqrt{\frac{0.0371(1-0.0371)}{3089}}} \sim N(0,1).
$$

---

So, to test the null hypothesis `\(H_0: \pi_{\text{Italy}} = 0.0371\)` vs. `\(H_A: \pi_{\text{Italy}} \ne 0.0371\)`, we need to find the probability of something *more extreme*. 

--




First, we find `\(Z_{\text{obs}}\)`:

$$
  Z_{\text{obs}} = \frac{\hat{p} - 0.0371}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}} = \frac{0.0346 - 0.0371}{\sqrt{\frac{0.0371(1-0.0371)}{3089}}} \approx -0.7351
$$

Then we want to find `\(P(\text{more extreme} | H_0 \text{ true})\)`. What is more extreme in this case? 
--
 Smaller than -0.7351 or greater than 0.7351. 
 
---

So, `\(\text{p-value} = P(Z &lt; -0.7351) + P(Z &gt; 0.7351) = 2\cdot P(Z &lt; -0.7351)\)`.

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-22-1.png" height="375px" style="display: block; margin: auto;" /&gt;

.pull-left[
```
Z &lt;- Normal(mu = 0, sigma = 1)

2*cdf(Z, -0.7351)
```

```
## [1] 0.4622786
```
]

--

.pull-right[
Since this is much larger than our `\(\alpha = 0.1\)`, we do not reject the idea that the mortality rates are the same in Italy and Mainland China.
]

---

`\(90\%\)` CIs for the risk in Mainland China and Italy:


```r
corona_virus %&gt;%
  filter(`Country/Region` %in% c("Italy", "China"),
         date == "2020-03-04") %&gt;% 
  group_by(`Country/Region`) %&gt;% 
  summarize_at(.vars = vars(confirmed, deaths, recovered),
               .funs = sum, na.rm = T) %&gt;% 
  mutate(P = deaths/confirmed,
         LL = P - quantile(Normal(), 0.95)*sqrt(P*(1-P)/confirmed),
         UL = P + quantile(Normal(), 0.95)*sqrt(P*(1-P)/confirmed))
```

```
## # A tibble: 2 x 7
##   `Country/Region` confirmed deaths recovered      P     LL     UL
## * &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 China                80386   2983     47927 0.0371 0.0360 0.0382
## 2 Italy                 3089    107       276 0.0346 0.0292 0.0401
```

---

What assumptions did we make that most likely are NOT true?

--

* Outcome of each confirmed case is a Bernoulli RV with some probability of success

--

* All independent

--

Other problems:

* When comparing rate in Italy to rate in Mainland China, we ignored that the rate in Mainland China was actually *estimated* from the data.

---

A map:


```r
world_map &lt;- map_data("world")

for_map &lt;- corona_virus %&gt;% 
  filter(date == "2020-03-04") %&gt;% 
  group_by(region = `Country/Region`) %&gt;% 
  summarize_at(vars(confirmed, deaths, recovered), sum) %&gt;% 
  ungroup() %&gt;% 
  mutate(region = case_when(region == "Mainland China" ~ "China",
                            region == "US" ~ "USA", 
                            TRUE ~ region))
```

---

A map:


```r
ggplot() +
  geom_map(data = left_join(world_map, for_map),
           map = world_map,
           aes(x = long, y = lat, map_id = region,
               fill = log(confirmed + 1))) +
  scale_fill_gradient2(limits = c(0, log(max(for_map$confirmed)+1)),
                       low = "white", high = "red") +
  coord_fixed() +
  labs(x = "", y = "") +
  guides(fill = guide_colorbar(title.position = "top",
                               barheight = 0.75, barwidth = 15)) +
  theme_void() +
  theme(legend.position = 'bottom',
        legend.margin = margin(0, 0, 0, 0),
        legend.box.margin = margin(0, 0, 0, 0),
        panel.background = element_rect(fill = "lightblue")) +
  ggtitle(label = "Confirmed Cases Of Corona Virus", 
          subtitle = max(corona_virus$date))
```


---

A map:

&lt;img src="lec12_slides_files/figure-html/unnamed-chunk-26-1.png" width="707px" style="display: block; margin: auto;" /&gt;

---




An animation:

&lt;center&gt;
  &lt;video controls width="707" height="400"&gt;
    &lt;source src="./covid-19.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/center&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ration": "16:10",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
