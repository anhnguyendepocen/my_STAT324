---
title: "Lecture 7: More Random Variables, and Intro to Estimation"
subtitle: "STAT 324"
author: "Ralph Trane"
institute: "University of Wisconsin--Madison<br><br>"
date: "Spring 2020"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [../css/uwmadison.css, default-fonts, ../css/extra-classes.css]
    lib_dir: libs
    nature:
      titleSlideClass: [center, top, .title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ration: '16:9'
      navigation:
        scroll: false
---

```{r include = FALSE}
knitr::opts_chunk$set(dpi = 300, fig.align = "center", message = FALSE, warning = FALSE)
```


# Misc

1. Announcement on Canvas

2. Show how to do tables in R Markdown

3. Recap

---

layout: true

# Random Variables: Example

**Binomial RV**

---

Based on several years of testing, it is determined that $96\%$ of circuit boards are fully operational. A warehouse contains a very large population of boards. If 4 are selected at random, the distribution of $X =$ the number of operational boards in that sample of 4 would be described by a binomial RV. 

Assumptions that must hold for $X \sim \text{Binomial}(n = 4, \pi = 0.96)$:

--

1. The "experiment" consists of 4 "sub-experiments" that each is a Bernoulli trial

  * the circuit board either works (1) or does not work (0)
  
--

2. The outcome of interest is the total number of successes in sub-experiments

--

3. The 4 "sub-experiments" are independent

  * By assumption

--

4. The probability of success for each "sub-experiment" is the same

  * each circuit board works with probability $0.96$

---
 
Question: a customer orders 4 circuit boards for a critical job. In order to be able to complete the job, they need at least 3 of the circuit boards to be fully functioning. What is the probability that they can finish the job?

--
 
Recall: if $Y \sim \text{Binomial}(n,\pi)$, then $P(Y = k) = {n \choose k}\pi^k(1-\pi)^{n-k}$. 

By hand:

$$\begin{aligned}
  P(X \ge 3) &= P(X = 3) + P(X = 4) \\
             &= {4 \choose 3} 0.96^3 \cdot (1 - 0.96)^1 + {4 \choose 4} 0.96^4 \cdot (1 - 0.96)^0 \\
             &= 4 \cdot `r round(0.96^3, digits = 3)`\cdot 0.04 + 1 \cdot 0.849 \cdot 1 \\
             &= `r 4*round(0.96^3, digits = 3)*0.04 + 0.849`
\end{aligned}$$


---
 
Question: a customer orders 4 circuit boards for a critical job. In order to be able to complete the job, they need at least 3 of the circuit boards to be fully functioning. What is the probability that they can finish the job?

Recall: if $Y \sim \text{Binomial}(n,\pi)$, then $P(Y = k) = {n \choose k}\pi^k(1-\pi)^{n-k}$. 

In `R`: remember, $P(X \ge 3) = 1 - P(X < 3) = 1 - P(X \le 2)$.

```{r binom-example, eval = T, message = FALSE, warning = FALSE}
library(distributions3)
X <- Binomial(size = 4, 
              p = 0.96)
```

.pull-left[
```{r}
1 - cdf(X, 2)
```
]

.pull-right[
```{r echo = T, message = FALSE, warning = FALSE}
sum(pmf(X, 3:4))
```
]

---

The factory manufactures approximately 928 circuit boards a day. What it the expected number of working circuit boards manufactured in a day?

--

$X =$ number of working circuit boards in a day. Then $X \sim \text{Binomial}(n = 928, \pi = 0.96)$. So,

--

$$
E(X) = n \cdot \pi = 928 \cdot 0.96 \approx `r round(928*0.96, digits = 3)`.
$$

--

What is the standard deviation of the number of working circuit boards manufactured in a day?

--

$$
\text{Var}(X) = n \cdot \pi \cdot (1 - \pi) = 928 \cdot 0.96 \cdot 0.04 \approx `r round(928*0.96*0.04, digits = 3)`
$$

So

$$
\text{SD}(X) = \sqrt{\text{Var}(X)} = \sqrt{`r round(928*0.96*0.04, digits = 3)`} = `r round(sqrt(round(928*0.96*0.04, digits = 3)), digits = 3)`.
$$

---

How many circuit boards can they with $98\%$ certainty say will be working on any given day?

--

I.e. what is the $x$ such that $P(X \ge x) = 0.98$? 

The p.m.f.:

```{r echo = FALSE, out.width = "800px", out.height = "300px", fig.width = 8}
library(tidyverse)
X <- Binomial(928, 0.96)
plotly::ggplotly(plot_pdf(X))
```

---

I.e., we want to find $x$ such that the area to the right is $0.98$:

```{r echo = FALSE, fig.width = 4, fig.height = 2.5, out.width = "400px", out.height = "250px"}
ggplot(data = tibble(x = 860:928, y = pdf(X, x)),
       aes(x = x, y = y, fill = x > 885)) +
  geom_bar(stat = "identity", color = "black", width = 1) + 
  scale_fill_manual(values = c("grey80", "grey40")) + 
  scale_x_continuous(breaks = c(860, 880, 885.5, 900, 920),
                     minor_breaks = c(870, 990, 910, 930),
                     labels = c(860, 880, "x", 900, 920)) +
  guides(fill = "none") +
  geom_vline(xintercept = 885.5, color = "darkred", size = 1) + 
  theme_bw()
```

--

Or, similarly, find $x$ so $P(X \le x) = 1 - 0.98 = 0.02$. So we want to find the 2nd percentile. We do this using the quantile function:

```{r}
X <- Binomial(928, 0.96)
quantile(X, 0.02)
```

---
layout: true

# Random Variables: Example

**Normal RV**

---
 
Recall: previously, we almost died in a secret chamber. Fortunately, we survived, and decided to get really into ants. 

Somehow, we know that the weights of the ants in our ant farm follow a normal distribution with mean $3$ mg and standard deviation $0.25$ mg. 

--

Weight of randomly chosen ant $= X \sim N(3, 0.25^2)$. 

---

```{r}
X <- Normal(mu = 3, sigma = 0.25)
```

```{r echo = F, fig.width = 3, fig.height = 2.5, out.width = "300px", out.height = "250px"}
ants_norm <- plot_pdf(X, limits = c(2, 4)) + labs(x = "Weights", y = "Density")
ants_norm
```

---

If we randomly select an ant, what is $P(X < 2.5)$? 

--

```{r echo = FALSE, fig.width = 3, fig.height = 2.5, out.width = "300px", out.height = "250px"}
ants_norm + geom_auc(to = 2.5)
```

```{r}
cdf(X, 2.5)
```

---

We know that ants are more likely to be closer to the expected value (i.e. closer to $3$ mg). In what interval will the weights of most (say, $95\%$) of the ants be?

--
 Middle $95\%$. So, what are $x_1$ and $x_2$ such that 

$$P(x_1 \le X \le x_2) = 0.95\%?$$

```{r echo = FALSE, fig.width = 3, fig.height = 2.5, out.width = "300px", out.height = "250px"}
ants_norm + geom_auc(from = 2.4, to = 3.6) + 
  geom_vline(xintercept = c(2.4, 3.6), linetype = "dashed", color = "darkred") +
  geom_text(data = data.frame(x = c(2.25, 3.75), 
                              y = c(0.2, 0.2),
                              label = c("x1", "x2"),
                              hjust = c(1.05,-0.05)),
            aes(label = label, hjust = hjust), 
            vjust = -0.1) +
  geom_segment(data = data.frame(x = c(2.25, 3.75),
                                 xend = c(2.4, 3.6),
                                 y = c(0.2, 0.2),
                                 yend = c(0,0)),
               aes(x = x, xend = xend, y = y, yend = yend),
               arrow = arrow(length = unit(0.1, "in")),
               color = "blue")
```

---

We know that ants are more likely to be closer to the expected value (i.e. closer to $3$ mg). In what interval will the weights of most (say, $95\%$) of the ants be? Middle $95\%$. So, what are $x_1$ and $x_2$ such that 

$$P(X < x_1) = P(X > x_2) = 2.5\%?$$

```{r echo = FALSE, fig.width = 3, fig.height = 2.5, out.width = "300px", out.height = "250px"}
ants_norm + 
  geom_auc(to = 2.4) + geom_auc(from = 3.6) + 
  geom_vline(xintercept = c(2.4, 3.6), linetype = "dashed", color = "darkred") +
  geom_text(data = data.frame(x = c(2.25, 3.75), 
                              y = c(0.2, 0.2),
                              label = c("x1", "x2"),
                              hjust = c(1.05,-0.05)),
            aes(label = label, hjust = hjust), 
            vjust = -0.1) +
  geom_segment(data = data.frame(x = c(2.25, 3.75),
                                 xend = c(2.4, 3.6),
                                 y = c(0.2, 0.2),
                                 yend = c(0,0)),
               aes(x = x, xend = xend, y = y, yend = yend),
               arrow = arrow(length = unit(0.1, "in")),
               color = "blue")
```

--

.pull-left[
```{r}
quantile(X, 0.025)
```
]

.pull-right[
```{r}
quantile(X, 1-0.025)
```
]

---
layout: false

# Random Variables: Symmetrical Distributions

Definition: Symmetrical Distribution = the curve to the left of the mean is a mirror image of the curve to the right of the mean.

--

An important, and extremely useful, fact about symmetrical distributions: $P(X < \mu - x) = P(X > \mu + x)$. 

**In words: if you move the same distance to the left and right of the mean, the area to the left and right, respectively, is the same!**

--

As a consequence, if $P(X < x_1) = P(X > x_2)$, then $|\mu - x_1| = |\mu - x_2|$. 

**In words: if the area (i.e. probability) to the left of one number is the same as the are to the right of another number, then the numbers are the same distance from the mean!**

--

In particular, if we are considering the standard normal, i.e. $X \sim N(0,1)$ (or any other symmetrical distribution with mean $0$):

$$
P(X < x_1) = P(X > x_2) \iff x_1 = -x_2.
$$

---
layout: false

# Random Variables: Summary/Vocab

" $X \sim$ " = " $X$ follows". This comes with:
* For discrete RV: Probability Mass Function (PMF) = $P(X = x)$ 

    - $P(X \text{ is something }) = \sum_x P(X = x) =$
--
 $1$

* For continuous RV: Probability Density Function (PDF) = "the curve"

    - Total area under curve = 
--
 $1$

--

- PMF/PDF allows us to calculate probabilities for all possible events


Generally,

* Cumulative Density Function (CDF) = $P(X \le x)$ 
    - "area under curve" to the left of $x$
    
* The $q$ that satisfies $P(X \le q) = p$ is called the $p$'th quantile
    - value that "cuts off" $p$ to the left

---
# Random Variables: Summary/Vocab

Normal Distribution:

* If $X \sim N(\mu_1, \sigma_1^2)$, $Y \sim N(\mu_2, \sigma_2^2)$ ( $X$, $Y$ independent )
    - $X \pm c \sim N(\mu_1 \pm c, \sigma_1^2)$, $c \cdot X \sim N(c \cdot \mu_1, c^2 \cdot \sigma_1^2)$
    - $X \pm Y \sim N(\mu_1 \pm \mu_2, \sigma_1^2 + \sigma_2^2)$

.pull-left[
* $P(\mu - \sigma < X < \mu + \sigma) \approx 0.68$

* $P(\mu - 2\sigma < X < \mu + 2\sigma) \approx 0.95$

* $P(\mu - 3\sigma < X < \mu + 3\sigma) \approx 0.998$

* $P(X < \mu - x ) = P(X > \mu + x)$ 

* $P(X < \mu) = P(X > \mu) = 0.5$
]

.pull-right[
```{r echo = FALSE, out.width = "300px", out.height = "200px", fig.height = 2, fig.width = 3}
x_labs <- c(paste0("\u03BC - ", 1:3, "\u03C3"),
            paste0("\u03BC + ", 1:3, "\u03C3"), 
            "\u03BC")

plot_pdf(Normal(), limits = c(-1, 1)*3.5) +
  geom_auc(aes(fill = "3"), 
           from = -3, to = 3) +
  geom_auc(aes(fill = "2"), 
           from = -2, to = 2) + 
  geom_auc(aes(fill = "1"), 
           from = -1, to = 1) +
  scale_x_continuous(breaks = c(c(-1,1)*rep(1:3, 2), 0), labels = x_labs,
                     minor_breaks = NULL) +
  scale_y_continuous(expand = c(mult = expand_scale(mult = c(0, 0.1)))) +
  scale_fill_manual("", values = viridis::viridis(3, alpha = 0.75)) +
  guides(fill = "none") +
  labs(x = "", y = "Density") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(hjust = 0, angle = -45, face = "bold"))
```

]

---
# Random Variables: Summary/Vocab

Normal Distribution:

* We call $Z \sim N(0,1)$ the *standard normal*

* Standardization: $\frac{X - E(X)}{\text{SD}(X)} = \frac{X-\mu_1}{\sigma_1} \sim$
--
 $N(0,1)$

--

**IMPORTANT NOTE: if you are ever asked to "standardize" something, it means "subtract the mean and divide by the standard deviation"!**

Also sometimes called "z-value"/"t-value" - will talk much, much, much more about this later. 

--

* The $z$ that satisfies $P(X \ge z) = \alpha$ is called the $\alpha$ critical value
    - value that "cuts off" $\alpha$ to the right
    - denoted $z_{\alpha}$.

---
# Normal RV: One more thing

How to check for normality? QQ-plots. 

```{r echo = FALSE, fig.width = 6, fig.height = 2, out.width = "600px"}
library(patchwork)
X <- Normal(mu = 10, sigma = 3)
Y <- Exponential(rate = 1)
Z <- Binomial(size = 4, p = 0.25)

different_samples <- data.frame(x = random(X, n = 100),
                                y = random(Y, n = 100),
                                z = random(Z, n = 100))
theme_set(theme_bw())

{
  ggplot(data = different_samples,
         aes(sample = x)) + 
    geom_qq() + 
    geom_abline(aes(slope = sd(x), intercept = mean(x))) + 
    ggtitle("Normal")
} + {
  ggplot(data = different_samples,
         aes(sample = y)) + 
    geom_qq() + 
    geom_abline(aes(slope = sd(y), intercept = mean(y))) + 
    ggtitle("Exponential")
} + {
  ggplot(data = different_samples,
         aes(sample = z)) + 
    geom_qq() +
    geom_abline(aes(slope = sd(z), intercept = mean(z))) + 
    ggtitle("Binomial")
}
```

```{r echo = FALSE, fig.width = 6, fig.height = 2, out.width = "600px"}
{
  plot_pdf(X) 
} + {
  plot_pdf(Y)
} + {
  plot_pdf(Z)
}
```



```{r eval = FALSE, echo = FALSE}
is <- sample(1:12)

## Simulate 11 data sets from normal with sample size 10, one from uniform(0,1)
many_small_normal <- data.frame(i = is[1:11]) %>% 
  mutate(distribution = "Normal",
         sample = map(i, random, d = X, n = 10)) %>% ## for each i, use random with d = X, n = 10
  unnest_longer(col = sample)

U <- Uniform()

uniform_sample <- data.frame(i = is[12]) %>% 
  mutate(Distribution = "Uniform",
         sample = map(i, random, d = U, n = 10)) %>% 
  unnest_longer(col = sample)

all_12_samples <- bind_rows(
  many_small_normal,
  uniform_sample
) %>% 
  group_by(i) %>% 
  mutate(m = mean(sample),
         s = sd(sample))

ggplot(data = all_12_samples,
       aes(sample = sample, group = i)) + 
  geom_qq() + 
  geom_abline(aes(intercept = m, slope = s)) + 
  facet_wrap(~i, ncol = 4, scales = "free") + 
  theme_bw() + 
  theme(axis.text = element_blank())
```


---
# Random Variables: Example

**Normal RVs**

Back to the ants: the weights of the ants in our ant farm follow a normal distribution with mean $3$ mg and standard deviation $0.25$ mg. 

Weight of randomly chosen ant = $X \sim N(3, 0.25^2)$.

$P(2.75 < X < 3.25)=$
--
 $0.68$ because $2.75 = \mu - \sigma$ and $3.25 = \mu + \sigma$.

--
 
$P(2.5 < X < 3.5)=$
--
 $0.95$ because $2.5 = \mu - 2\sigma$ and $3.5 = \mu + 2\sigma$.


---
layout: true

# Estimation

---

The art of coming up with our "best guess" for the truth. This is called *an estimate*. 

An *estimator* is a function that takes values from a sample and provides an estimate ("best guess"). 

---

In order to come up with a good estimator, it is important to know how the sample was gathered. Three important definitions:

* a sample is called a **simple random sample** (SRS) if every possible element is equally likely to be sampled
    - unless otherwise stated, **all samples in this class are SRS**

* a sample is drawn **with replacement** if an element is replaced to the population before the next element is drawn. Otherwise, we say it is drawn **without replacement**.
    - without replacement = each element can only be sampled once.

* a collection of RVs $X_1, X_2, ..., X_n$ are said to be **independent and identically distributed** (iid) if
    1. they are all independent of each other
    2. they all follow the same distribution.

--

Technically, SRS **ONLY** if done with replacement. However, if population is "big enough", sample without repleacement is approximately the same as with replacement. (Recall last weeks discussion.)

---
layout: true

# Estimation

**Estimating Population Mean**

---

* A car manufacturer uses an automatic device to apply paint to engine blocks. 
* engine blocks get very hot, so the paint must be heat-resistant, 
* important that the amount applied is of a minimum thickness
* warehouse contains thousands of blocks painted by the automatic device
* he manufacturer wants to know the average amount of paint applied by the device

--

16 blocks will be selected at random, and the paint thickness measured in mm. Let $X_1, ..., X_{16}$ be RVs indicating the thickness of the 16 blocks. 

Let's assume these RVs are iid -- i.e. independent and identically distributed. There exists some true expected value of these: $E(X_i) = \mu$. There also exists some true variance, $\text{Var}(X_i) = \sigma^2$.

---

Now we actually observe 16 realizations of these RVs: 

```{r}
paint_thickness <- data.frame(
  thickness = c(1.29, 1.12, 0.88, 1.65, 1.48, 1.59, 1.04, 0.83, 
                1.76, 1.31, 0.88, 1.71, 1.83, 1.09, 1.62, 1.49)
)
```

Using these data, what would be your "best guess" for the true mean $\mu$? 

--

I would use the sample average: 

```{r}
paint_thickness %>% 
  summarize(Mean = mean(thickness))
```

This **estimATE** comes from the **estimatOR**: $\hat{\mu} = \bar{X} = \frac{1}{n} \sum_{i = 1}^n X_i$.

--
 
Notice: the **<span class="red">estimator</span>** is a
--
 .red[**RV**] while the **<span class="blue">estimate</span>** is a
--
 **<span class="blue">realization</span>** of that RV.
